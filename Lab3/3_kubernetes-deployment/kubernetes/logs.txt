* 
* ==> Audit <==
* |---------|-----------------------------|----------------|--------------|---------|---------------------|---------------------|
| Command |            Args             |    Profile     |     User     | Version |     Start Time      |      End Time       |
|---------|-----------------------------|----------------|--------------|---------|---------------------|---------------------|
| start   | --nodes 2 -p multinode-demo | multinode-demo | athikafatima | v1.32.0 | 18 Jan 24 21:29 EST |                     |
| start   | --nodes 2 -p multinode-demo | multinode-demo | athikafatima | v1.32.0 | 18 Jan 24 21:31 EST |                     |
| start   | --nodes 2 -p multinode-demo | multinode-demo | athikafatima | v1.32.0 | 18 Jan 24 21:42 EST |                     |
| stop    |                             | minikube       | athikafatima | v1.32.0 | 18 Jan 24 21:48 EST |                     |
| start   | --nodes 2 -p multinode-demo | multinode-demo | athikafatima | v1.32.0 | 18 Jan 24 22:01 EST |                     |
| start   | --nodes 2 -p multinode-demo | multinode-demo | athikafatima | v1.32.0 | 18 Jan 24 22:01 EST |                     |
| start   | --nodes 2 -p multinode-demo | multinode-demo | athikafatima | v1.32.0 | 18 Jan 24 22:06 EST |                     |
| start   |                             | minikube       | athikafatima | v1.32.0 | 18 Jan 24 22:08 EST | 18 Jan 24 22:09 EST |
| start   | --nodes 2 -p multinode-demo | multinode-demo | athikafatima | v1.32.0 | 19 Jan 24 18:54 EST |                     |
| start   |                             | minikube       | athikafatima | v1.32.0 | 19 Jan 24 19:08 EST | 19 Jan 24 19:09 EST |
| service | note-service                | minikube       | athikafatima | v1.32.0 | 19 Jan 24 19:11 EST |                     |
| service | mongo-express-service       | minikube       | athikafatima | v1.32.0 | 19 Jan 24 19:13 EST |                     |
|---------|-----------------------------|----------------|--------------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2024/01/19 19:08:24
Running on machine: Athikas-MacBook-Pro
Binary: Built with gc go1.21.3 for darwin/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0119 19:08:24.491477   15922 out.go:296] Setting OutFile to fd 1 ...
I0119 19:08:24.491906   15922 out.go:348] isatty.IsTerminal(1) = true
I0119 19:08:24.491910   15922 out.go:309] Setting ErrFile to fd 2...
I0119 19:08:24.491915   15922 out.go:348] isatty.IsTerminal(2) = true
I0119 19:08:24.492154   15922 root.go:338] Updating PATH: /Users/athikafatima/.minikube/bin
W0119 19:08:24.492261   15922 root.go:314] Error reading config file at /Users/athikafatima/.minikube/config/config.json: open /Users/athikafatima/.minikube/config/config.json: no such file or directory
I0119 19:08:24.495415   15922 out.go:303] Setting JSON to false
I0119 19:08:24.536205   15922 start.go:128] hostinfo: {"hostname":"Athikas-MacBook-Pro.local","uptime":856089,"bootTime":1704853215,"procs":491,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"14.1.2","kernelVersion":"23.1.0","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"79bb3c52-065c-5db6-9b70-71c6ab7d7765"}
W0119 19:08:24.536315   15922 start.go:136] gopshost.Virtualization returned error: not implemented yet
I0119 19:08:24.557333   15922 out.go:177] üòÑ  minikube v1.32.0 on Darwin 14.1.2
I0119 19:08:24.594686   15922 notify.go:220] Checking for updates...
I0119 19:08:24.596631   15922 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0119 19:08:24.597062   15922 driver.go:378] Setting default libvirt URI to qemu:///system
I0119 19:08:24.695520   15922 docker.go:122] docker version: linux-24.0.6:Docker Desktop 4.24.0 (122432)
I0119 19:08:24.695693   15922 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0119 19:08:25.091677   15922 info.go:266] docker info: {ID:a429f5f2-be4e-4acd-8ea7-2643a9c0e1ef Containers:7 ContainersRunning:0 ContainersPaused:0 ContainersStopped:7 Images:23 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:44 OomKillDisable:false NGoroutines:68 SystemTime:2024-01-20 00:06:47.597050477 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:12 KernelVersion:6.4.16-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:9 MemTotal:4120907776 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.6 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:8165feabfdfe38c65b599c4993d227328c231fca Expected:8165feabfdfe38c65b599c4993d227328c231fca} RuncCommit:{ID:v1.1.8-0-g82f18fe Expected:v1.1.8-0-g82f18fe} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/Users/athikafatima/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2-desktop.5] map[Name:compose Path:/Users/athikafatima/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.22.0-desktop.2] map[Name:dev Path:/Users/athikafatima/.docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:/Users/athikafatima/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.20] map[Name:init Path:/Users/athikafatima/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.8] map[Name:sbom Path:/Users/athikafatima/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:/Users/athikafatima/.docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:/Users/athikafatima/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.0.7]] Warnings:<nil>}}
I0119 19:08:25.109231   15922 out.go:177] ‚ú®  Using the docker driver based on existing profile
I0119 19:08:25.128447   15922 start.go:298] selected driver: docker
I0119 19:08:25.128482   15922 start.go:902] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0119 19:08:25.128657   15922 start.go:913] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0119 19:08:25.128943   15922 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0119 19:08:25.277367   15922 info.go:266] docker info: {ID:a429f5f2-be4e-4acd-8ea7-2643a9c0e1ef Containers:7 ContainersRunning:0 ContainersPaused:0 ContainersStopped:7 Images:23 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:44 OomKillDisable:false NGoroutines:68 SystemTime:2024-01-20 00:06:47.597050477 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:12 KernelVersion:6.4.16-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:9 MemTotal:4120907776 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.6 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:8165feabfdfe38c65b599c4993d227328c231fca Expected:8165feabfdfe38c65b599c4993d227328c231fca} RuncCommit:{ID:v1.1.8-0-g82f18fe Expected:v1.1.8-0-g82f18fe} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/Users/athikafatima/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2-desktop.5] map[Name:compose Path:/Users/athikafatima/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.22.0-desktop.2] map[Name:dev Path:/Users/athikafatima/.docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:/Users/athikafatima/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.20] map[Name:init Path:/Users/athikafatima/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.8] map[Name:sbom Path:/Users/athikafatima/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:/Users/athikafatima/.docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:/Users/athikafatima/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.0.7]] Warnings:<nil>}}
I0119 19:08:25.282764   15922 cni.go:84] Creating CNI manager for ""
I0119 19:08:25.282797   15922 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0119 19:08:25.282814   15922 start_flags.go:323] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0119 19:08:25.300476   15922 out.go:177] üëç  Starting control plane node minikube in cluster minikube
I0119 19:08:25.339306   15922 cache.go:121] Beginning downloading kic base image for docker with docker
I0119 19:08:25.358244   15922 out.go:177] üöú  Pulling base image ...
I0119 19:08:25.395335   15922 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0119 19:08:25.395433   15922 preload.go:148] Found local preload: /Users/athikafatima/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4
I0119 19:08:25.395460   15922 cache.go:56] Caching tarball of preloaded images
I0119 19:08:25.395534   15922 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon
I0119 19:08:25.395872   15922 preload.go:174] Found /Users/athikafatima/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0119 19:08:25.395897   15922 cache.go:59] Finished verifying existence of preloaded tar for  v1.28.3 on docker
I0119 19:08:25.396206   15922 profile.go:148] Saving config to /Users/athikafatima/.minikube/profiles/minikube/config.json ...
I0119 19:08:25.477765   15922 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon, skipping pull
I0119 19:08:25.477790   15922 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 exists in daemon, skipping load
I0119 19:08:25.477810   15922 cache.go:194] Successfully downloaded all kic artifacts
I0119 19:08:25.477876   15922 start.go:365] acquiring machines lock for minikube: {Name:mkf4b3c30dae118e57f76ba4e9665c409cc51241 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0119 19:08:25.477990   15922 start.go:369] acquired machines lock for "minikube" in 95.606¬µs
I0119 19:08:25.478716   15922 start.go:96] Skipping create...Using existing machine configuration
I0119 19:08:25.479542   15922 fix.go:54] fixHost starting: 
I0119 19:08:25.479774   15922 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0119 19:08:25.543265   15922 fix.go:102] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0119 19:08:25.543326   15922 fix.go:128] unexpected machine state, will restart: <nil>
I0119 19:08:25.562325   15922 out.go:177] üîÑ  Restarting existing docker container for "minikube" ...
I0119 19:08:25.581639   15922 cli_runner.go:164] Run: docker start minikube
I0119 19:08:35.311026   15922 cli_runner.go:217] Completed: docker start minikube: (9.729415605s)
I0119 19:08:35.312255   15922 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0119 19:08:35.397270   15922 kic.go:430] container "minikube" state is running.
I0119 19:08:35.399501   15922 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0119 19:08:35.484895   15922 profile.go:148] Saving config to /Users/athikafatima/.minikube/profiles/minikube/config.json ...
I0119 19:08:35.486163   15922 machine.go:88] provisioning docker machine ...
I0119 19:08:35.487772   15922 ubuntu.go:169] provisioning hostname "minikube"
I0119 19:08:35.488634   15922 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0119 19:08:35.614903   15922 main.go:141] libmachine: Using SSH client type: native
I0119 19:08:35.618797   15922 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1405ca0] 0x1408980 <nil>  [] 0s} 127.0.0.1 53550 <nil> <nil>}
I0119 19:08:35.618829   15922 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0119 19:08:35.622909   15922 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I0119 19:08:38.859135   15922 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0119 19:08:38.859699   15922 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0119 19:08:38.976565   15922 main.go:141] libmachine: Using SSH client type: native
I0119 19:08:38.977145   15922 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1405ca0] 0x1408980 <nil>  [] 0s} 127.0.0.1 53550 <nil> <nil>}
I0119 19:08:38.977159   15922 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0119 19:08:39.126188   15922 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0119 19:08:39.126221   15922 ubuntu.go:175] set auth options {CertDir:/Users/athikafatima/.minikube CaCertPath:/Users/athikafatima/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/athikafatima/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/athikafatima/.minikube/machines/server.pem ServerKeyPath:/Users/athikafatima/.minikube/machines/server-key.pem ClientKeyPath:/Users/athikafatima/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/athikafatima/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/athikafatima/.minikube}
I0119 19:08:39.126276   15922 ubuntu.go:177] setting up certificates
I0119 19:08:39.131665   15922 provision.go:83] configureAuth start
I0119 19:08:39.131813   15922 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0119 19:08:39.209287   15922 provision.go:138] copyHostCerts
I0119 19:08:39.245956   15922 exec_runner.go:144] found /Users/athikafatima/.minikube/ca.pem, removing ...
I0119 19:08:39.252603   15922 exec_runner.go:203] rm: /Users/athikafatima/.minikube/ca.pem
I0119 19:08:39.259220   15922 exec_runner.go:151] cp: /Users/athikafatima/.minikube/certs/ca.pem --> /Users/athikafatima/.minikube/ca.pem (1094 bytes)
I0119 19:08:39.278689   15922 exec_runner.go:144] found /Users/athikafatima/.minikube/cert.pem, removing ...
I0119 19:08:39.278711   15922 exec_runner.go:203] rm: /Users/athikafatima/.minikube/cert.pem
I0119 19:08:39.279075   15922 exec_runner.go:151] cp: /Users/athikafatima/.minikube/certs/cert.pem --> /Users/athikafatima/.minikube/cert.pem (1135 bytes)
I0119 19:08:39.284887   15922 exec_runner.go:144] found /Users/athikafatima/.minikube/key.pem, removing ...
I0119 19:08:39.284923   15922 exec_runner.go:203] rm: /Users/athikafatima/.minikube/key.pem
I0119 19:08:39.285303   15922 exec_runner.go:151] cp: /Users/athikafatima/.minikube/certs/key.pem --> /Users/athikafatima/.minikube/key.pem (1679 bytes)
I0119 19:08:39.291109   15922 provision.go:112] generating server cert: /Users/athikafatima/.minikube/machines/server.pem ca-key=/Users/athikafatima/.minikube/certs/ca.pem private-key=/Users/athikafatima/.minikube/certs/ca-key.pem org=athikafatima.minikube san=[192.168.58.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0119 19:08:39.798982   15922 provision.go:172] copyRemoteCerts
I0119 19:08:39.800401   15922 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0119 19:08:39.800473   15922 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0119 19:08:39.888983   15922 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53550 SSHKeyPath:/Users/athikafatima/.minikube/machines/minikube/id_rsa Username:docker}
I0119 19:08:40.001681   15922 ssh_runner.go:362] scp /Users/athikafatima/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1094 bytes)
I0119 19:08:40.042363   15922 ssh_runner.go:362] scp /Users/athikafatima/.minikube/machines/server.pem --> /etc/docker/server.pem (1216 bytes)
I0119 19:08:40.071491   15922 ssh_runner.go:362] scp /Users/athikafatima/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0119 19:08:40.105749   15922 provision.go:86] duration metric: configureAuth took 974.070378ms
I0119 19:08:40.105764   15922 ubuntu.go:193] setting minikube options for container-runtime
I0119 19:08:40.105986   15922 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0119 19:08:40.106050   15922 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0119 19:08:40.203845   15922 main.go:141] libmachine: Using SSH client type: native
I0119 19:08:40.204286   15922 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1405ca0] 0x1408980 <nil>  [] 0s} 127.0.0.1 53550 <nil> <nil>}
I0119 19:08:40.204301   15922 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0119 19:08:40.345282   15922 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0119 19:08:40.345292   15922 ubuntu.go:71] root file system type: overlay
I0119 19:08:40.350513   15922 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0119 19:08:40.350601   15922 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0119 19:08:40.427183   15922 main.go:141] libmachine: Using SSH client type: native
I0119 19:08:40.427582   15922 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1405ca0] 0x1408980 <nil>  [] 0s} 127.0.0.1 53550 <nil> <nil>}
I0119 19:08:40.427645   15922 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0119 19:08:40.578138   15922 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0119 19:08:40.578670   15922 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0119 19:08:40.654062   15922 main.go:141] libmachine: Using SSH client type: native
I0119 19:08:40.654444   15922 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1405ca0] 0x1408980 <nil>  [] 0s} 127.0.0.1 53550 <nil> <nil>}
I0119 19:08:40.654456   15922 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0119 19:08:40.796168   15922 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0119 19:08:40.796189   15922 machine.go:91] provisioned docker machine in 5.310103145s
I0119 19:08:40.796197   15922 start.go:300] post-start starting for "minikube" (driver="docker")
I0119 19:08:40.796609   15922 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0119 19:08:40.796736   15922 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0119 19:08:40.796819   15922 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0119 19:08:40.871575   15922 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53550 SSHKeyPath:/Users/athikafatima/.minikube/machines/minikube/id_rsa Username:docker}
I0119 19:08:40.978536   15922 ssh_runner.go:195] Run: cat /etc/os-release
I0119 19:08:40.987483   15922 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0119 19:08:40.987521   15922 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0119 19:08:40.987543   15922 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0119 19:08:40.987552   15922 info.go:137] Remote host: Ubuntu 22.04.3 LTS
I0119 19:08:40.987922   15922 filesync.go:126] Scanning /Users/athikafatima/.minikube/addons for local assets ...
I0119 19:08:40.988105   15922 filesync.go:126] Scanning /Users/athikafatima/.minikube/files for local assets ...
I0119 19:08:40.988164   15922 start.go:303] post-start completed in 191.964839ms
I0119 19:08:40.988304   15922 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0119 19:08:40.988373   15922 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0119 19:08:41.066875   15922 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53550 SSHKeyPath:/Users/athikafatima/.minikube/machines/minikube/id_rsa Username:docker}
I0119 19:08:41.172546   15922 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0119 19:08:41.184009   15922 fix.go:56] fixHost completed within 15.705299389s
I0119 19:08:41.184053   15922 start.go:83] releasing machines lock for "minikube", held for 15.706301815s
I0119 19:08:41.184238   15922 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0119 19:08:41.297532   15922 ssh_runner.go:195] Run: cat /version.json
I0119 19:08:41.297622   15922 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0119 19:08:41.302090   15922 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0119 19:08:41.312325   15922 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0119 19:08:41.467901   15922 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53550 SSHKeyPath:/Users/athikafatima/.minikube/machines/minikube/id_rsa Username:docker}
I0119 19:08:41.491600   15922 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53550 SSHKeyPath:/Users/athikafatima/.minikube/machines/minikube/id_rsa Username:docker}
I0119 19:08:41.572497   15922 ssh_runner.go:195] Run: systemctl --version
I0119 19:08:41.906989   15922 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0119 19:08:41.920158   15922 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0119 19:08:41.950876   15922 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0119 19:08:41.951001   15922 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0119 19:08:41.973961   15922 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0119 19:08:41.973981   15922 start.go:472] detecting cgroup driver to use...
I0119 19:08:41.974937   15922 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0119 19:08:41.979930   15922 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0119 19:08:42.004819   15922 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0119 19:08:42.019552   15922 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0119 19:08:42.033546   15922 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I0119 19:08:42.033639   15922 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0119 19:08:42.046987   15922 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0119 19:08:42.060133   15922 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0119 19:08:42.073625   15922 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0119 19:08:42.086838   15922 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0119 19:08:42.099823   15922 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0119 19:08:42.113437   15922 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0119 19:08:42.127037   15922 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0119 19:08:42.138394   15922 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0119 19:08:42.214445   15922 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0119 19:08:42.344368   15922 start.go:472] detecting cgroup driver to use...
I0119 19:08:42.344406   15922 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0119 19:08:42.345293   15922 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0119 19:08:42.364326   15922 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0119 19:08:42.364438   15922 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0119 19:08:42.389705   15922 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0119 19:08:42.422023   15922 ssh_runner.go:195] Run: which cri-dockerd
I0119 19:08:42.430250   15922 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0119 19:08:42.445223   15922 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0119 19:08:42.472232   15922 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0119 19:08:42.652670   15922 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0119 19:08:42.757517   15922 docker.go:560] configuring docker to use "cgroupfs" as cgroup driver...
I0119 19:08:42.757722   15922 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0119 19:08:42.780404   15922 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0119 19:08:42.883961   15922 ssh_runner.go:195] Run: sudo systemctl restart docker
I0119 19:08:43.247191   15922 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0119 19:08:43.319501   15922 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0119 19:08:43.388742   15922 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0119 19:08:43.460465   15922 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0119 19:08:43.532992   15922 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0119 19:08:43.563805   15922 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0119 19:08:43.636553   15922 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I0119 19:08:44.023930   15922 start.go:519] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0119 19:08:44.024458   15922 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0119 19:08:44.032682   15922 start.go:540] Will wait 60s for crictl version
I0119 19:08:44.032782   15922 ssh_runner.go:195] Run: which crictl
I0119 19:08:44.039536   15922 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0119 19:08:44.340259   15922 start.go:556] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.7
RuntimeApiVersion:  v1
I0119 19:08:44.340325   15922 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0119 19:08:44.509930   15922 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0119 19:08:44.574901   15922 out.go:204] üê≥  Preparing Kubernetes v1.28.3 on Docker 24.0.7 ...
I0119 19:08:44.575998   15922 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0119 19:08:45.311434   15922 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0119 19:08:45.312725   15922 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0119 19:08:45.325176   15922 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0119 19:08:45.342656   15922 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0119 19:08:45.439358   15922 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0119 19:08:45.439422   15922 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0119 19:08:45.492657   15922 docker.go:671] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0119 19:08:45.493594   15922 docker.go:601] Images already preloaded, skipping extraction
I0119 19:08:45.493961   15922 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0119 19:08:45.536061   15922 docker.go:671] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0119 19:08:45.536691   15922 cache_images.go:84] Images are preloaded, skipping loading
I0119 19:08:45.537517   15922 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0119 19:08:45.887155   15922 cni.go:84] Creating CNI manager for ""
I0119 19:08:45.887195   15922 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0119 19:08:45.887973   15922 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0119 19:08:45.888187   15922 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.58.2 APIServerPort:8443 KubernetesVersion:v1.28.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.58.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.58.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0119 19:08:45.888469   15922 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.58.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.58.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.58.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.28.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0119 19:08:45.889311   15922 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.28.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.58.2

[Install]
 config:
{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0119 19:08:45.889419   15922 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.28.3
I0119 19:08:45.902874   15922 binaries.go:44] Found k8s binaries, skipping transfer
I0119 19:08:45.902979   15922 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0119 19:08:45.914271   15922 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I0119 19:08:45.942253   15922 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0119 19:08:45.963896   15922 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2091 bytes)
I0119 19:08:45.986361   15922 ssh_runner.go:195] Run: grep 192.168.58.2	control-plane.minikube.internal$ /etc/hosts
I0119 19:08:45.992313   15922 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.58.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0119 19:08:46.007755   15922 certs.go:56] Setting up /Users/athikafatima/.minikube/profiles/minikube for IP: 192.168.58.2
I0119 19:08:46.007779   15922 certs.go:190] acquiring lock for shared ca certs: {Name:mk7e1b11923a876f8d5910aa6d56721725a9e001 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0119 19:08:46.009198   15922 certs.go:199] skipping minikubeCA CA generation: /Users/athikafatima/.minikube/ca.key
I0119 19:08:46.009601   15922 certs.go:199] skipping proxyClientCA CA generation: /Users/athikafatima/.minikube/proxy-client-ca.key
I0119 19:08:46.010286   15922 certs.go:315] skipping minikube-user signed cert generation: /Users/athikafatima/.minikube/profiles/minikube/client.key
I0119 19:08:46.010659   15922 certs.go:315] skipping minikube signed cert generation: /Users/athikafatima/.minikube/profiles/minikube/apiserver.key.cee25041
I0119 19:08:46.011094   15922 certs.go:315] skipping aggregator signed cert generation: /Users/athikafatima/.minikube/profiles/minikube/proxy-client.key
I0119 19:08:46.011352   15922 certs.go:437] found cert: /Users/athikafatima/.minikube/certs/Users/athikafatima/.minikube/certs/ca-key.pem (1679 bytes)
I0119 19:08:46.011737   15922 certs.go:437] found cert: /Users/athikafatima/.minikube/certs/Users/athikafatima/.minikube/certs/ca.pem (1094 bytes)
I0119 19:08:46.011775   15922 certs.go:437] found cert: /Users/athikafatima/.minikube/certs/Users/athikafatima/.minikube/certs/cert.pem (1135 bytes)
I0119 19:08:46.012088   15922 certs.go:437] found cert: /Users/athikafatima/.minikube/certs/Users/athikafatima/.minikube/certs/key.pem (1679 bytes)
I0119 19:08:46.018118   15922 ssh_runner.go:362] scp /Users/athikafatima/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0119 19:08:46.054300   15922 ssh_runner.go:362] scp /Users/athikafatima/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0119 19:08:46.083510   15922 ssh_runner.go:362] scp /Users/athikafatima/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0119 19:08:46.113433   15922 ssh_runner.go:362] scp /Users/athikafatima/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0119 19:08:46.149285   15922 ssh_runner.go:362] scp /Users/athikafatima/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0119 19:08:46.177861   15922 ssh_runner.go:362] scp /Users/athikafatima/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0119 19:08:46.207308   15922 ssh_runner.go:362] scp /Users/athikafatima/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0119 19:08:46.244065   15922 ssh_runner.go:362] scp /Users/athikafatima/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0119 19:08:46.272879   15922 ssh_runner.go:362] scp /Users/athikafatima/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0119 19:08:46.304938   15922 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0119 19:08:46.335103   15922 ssh_runner.go:195] Run: openssl version
I0119 19:08:46.351213   15922 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0119 19:08:46.387564   15922 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0119 19:08:46.399788   15922 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Jan 19 03:09 /usr/share/ca-certificates/minikubeCA.pem
I0119 19:08:46.399876   15922 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0119 19:08:46.415178   15922 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0119 19:08:46.448316   15922 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I0119 19:08:46.456187   15922 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0119 19:08:46.473473   15922 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0119 19:08:46.485671   15922 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0119 19:08:46.500694   15922 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0119 19:08:46.520452   15922 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0119 19:08:46.549446   15922 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0119 19:08:46.564824   15922 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0119 19:08:46.564954   15922 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0119 19:08:46.596927   15922 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0119 19:08:46.609245   15922 kubeadm.go:419] found existing configuration files, will attempt cluster restart
I0119 19:08:46.614117   15922 kubeadm.go:636] restartCluster start
I0119 19:08:46.614212   15922 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0119 19:08:46.628867   15922 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0119 19:08:46.628948   15922 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0119 19:08:46.815239   15922 kubeconfig.go:92] found "minikube" server: "https://127.0.0.1:51216"
I0119 19:08:46.815281   15922 kubeconfig.go:135] verify returned: got: 127.0.0.1:51216, want: 127.0.0.1:53549
I0119 19:08:46.816402   15922 lock.go:35] WriteFile acquiring /Users/athikafatima/.kube/config: {Name:mk12c0c56e95c033a7ed2a0a9425636f9df0fdb9 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0119 19:08:46.843748   15922 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0119 19:08:46.862674   15922 api_server.go:166] Checking apiserver status ...
I0119 19:08:46.862755   15922 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0119 19:08:46.881546   15922 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0119 19:08:46.881559   15922 api_server.go:166] Checking apiserver status ...
I0119 19:08:46.881651   15922 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0119 19:08:46.895636   15922 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0119 19:08:47.396484   15922 api_server.go:166] Checking apiserver status ...
I0119 19:08:47.396589   15922 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0119 19:08:47.412721   15922 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0119 19:08:47.896161   15922 api_server.go:166] Checking apiserver status ...
I0119 19:08:47.896408   15922 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0119 19:08:47.924667   15922 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0119 19:08:48.396896   15922 api_server.go:166] Checking apiserver status ...
I0119 19:08:48.397141   15922 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0119 19:08:48.423499   15922 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0119 19:08:48.896517   15922 api_server.go:166] Checking apiserver status ...
I0119 19:08:48.897076   15922 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0119 19:08:48.958968   15922 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0119 19:08:49.395953   15922 api_server.go:166] Checking apiserver status ...
I0119 19:08:49.396198   15922 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0119 19:08:49.419596   15922 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0119 19:08:49.896195   15922 api_server.go:166] Checking apiserver status ...
I0119 19:08:49.954558   15922 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0119 19:08:49.969787   15922 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0119 19:08:50.400211   15922 api_server.go:166] Checking apiserver status ...
I0119 19:08:50.400377   15922 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0119 19:08:50.472016   15922 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0119 19:08:50.896700   15922 api_server.go:166] Checking apiserver status ...
I0119 19:08:50.896890   15922 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0119 19:08:50.926832   15922 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0119 19:08:51.396591   15922 api_server.go:166] Checking apiserver status ...
I0119 19:08:51.396780   15922 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0119 19:08:51.423084   15922 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0119 19:08:51.895937   15922 api_server.go:166] Checking apiserver status ...
I0119 19:08:51.896169   15922 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0119 19:08:51.918448   15922 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0119 19:08:52.396325   15922 api_server.go:166] Checking apiserver status ...
I0119 19:08:52.396552   15922 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0119 19:08:52.418430   15922 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0119 19:08:52.895904   15922 api_server.go:166] Checking apiserver status ...
I0119 19:08:52.896011   15922 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0119 19:08:52.912471   15922 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0119 19:08:53.396656   15922 api_server.go:166] Checking apiserver status ...
I0119 19:08:53.396938   15922 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0119 19:08:53.422901   15922 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0119 19:08:53.895882   15922 api_server.go:166] Checking apiserver status ...
I0119 19:08:53.895999   15922 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0119 19:08:53.917987   15922 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0119 19:08:54.396733   15922 api_server.go:166] Checking apiserver status ...
I0119 19:08:54.396927   15922 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0119 19:08:54.419637   15922 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0119 19:08:54.896681   15922 api_server.go:166] Checking apiserver status ...
I0119 19:08:54.896861   15922 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0119 19:08:54.914986   15922 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0119 19:08:55.396763   15922 api_server.go:166] Checking apiserver status ...
I0119 19:08:55.397272   15922 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0119 19:08:55.415658   15922 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0119 19:08:55.896659   15922 api_server.go:166] Checking apiserver status ...
I0119 19:08:55.896929   15922 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0119 19:08:55.923771   15922 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0119 19:08:56.396623   15922 api_server.go:166] Checking apiserver status ...
I0119 19:08:56.396818   15922 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0119 19:08:56.422833   15922 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0119 19:08:56.863787   15922 kubeadm.go:611] needs reconfigure: apiserver error: context deadline exceeded
I0119 19:08:56.864510   15922 kubeadm.go:1128] stopping kube-system containers ...
I0119 19:08:56.864713   15922 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0119 19:08:56.914819   15922 docker.go:469] Stopping containers: [109ce8d6b57b de3576ca4d2d 2cae5dd92509 6eb1ba2a24ba 8b19fc1e7fc7 6f0ca9033a94 130ce2c4a90e b1f1cf24b4c7 fbcaf7940993 bf65849b8305 28d5cd0baa7a e50648fc1c4c b7b652350db6 c11d5b7c1c02 d6e725046f44]
I0119 19:08:56.915010   15922 ssh_runner.go:195] Run: docker stop 109ce8d6b57b de3576ca4d2d 2cae5dd92509 6eb1ba2a24ba 8b19fc1e7fc7 6f0ca9033a94 130ce2c4a90e b1f1cf24b4c7 fbcaf7940993 bf65849b8305 28d5cd0baa7a e50648fc1c4c b7b652350db6 c11d5b7c1c02 d6e725046f44
I0119 19:08:56.945088   15922 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I0119 19:08:56.971204   15922 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0119 19:08:56.989442   15922 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5643 Jan 19 03:09 /etc/kubernetes/admin.conf
-rw------- 1 root root 5656 Jan 19 03:09 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 1971 Jan 19 03:09 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5600 Jan 19 03:09 /etc/kubernetes/scheduler.conf

I0119 19:08:56.989597   15922 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0119 19:08:57.007655   15922 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0119 19:08:57.025432   15922 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0119 19:08:57.038154   15922 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I0119 19:08:57.038239   15922 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0119 19:08:57.049601   15922 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0119 19:08:57.062287   15922 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I0119 19:08:57.062825   15922 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0119 19:08:57.075356   15922 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0119 19:08:57.087823   15922 kubeadm.go:713] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I0119 19:08:57.087833   15922 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0119 19:08:57.371080   15922 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I0119 19:08:58.490083   15922 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml": (1.119004482s)
I0119 19:08:58.490096   15922 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I0119 19:08:58.852182   15922 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I0119 19:08:59.003191   15922 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I0119 19:08:59.097764   15922 api_server.go:52] waiting for apiserver process to appear ...
I0119 19:08:59.097881   15922 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0119 19:08:59.114687   15922 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0119 19:08:59.629879   15922 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0119 19:09:00.129215   15922 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0119 19:09:00.629110   15922 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0119 19:09:01.129226   15922 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0119 19:09:01.630199   15922 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0119 19:09:02.129173   15922 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0119 19:09:02.163558   15922 api_server.go:72] duration metric: took 3.065834352s to wait for apiserver process to appear ...
I0119 19:09:02.163598   15922 api_server.go:88] waiting for apiserver healthz status ...
I0119 19:09:02.167265   15922 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53549/healthz ...
I0119 19:09:02.176426   15922 api_server.go:269] stopped: https://127.0.0.1:53549/healthz: Get "https://127.0.0.1:53549/healthz": EOF
I0119 19:09:02.176498   15922 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53549/healthz ...
I0119 19:09:02.179824   15922 api_server.go:269] stopped: https://127.0.0.1:53549/healthz: Get "https://127.0.0.1:53549/healthz": EOF
I0119 19:09:02.679937   15922 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53549/healthz ...
I0119 19:09:02.682302   15922 api_server.go:269] stopped: https://127.0.0.1:53549/healthz: Get "https://127.0.0.1:53549/healthz": EOF
I0119 19:09:03.180217   15922 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53549/healthz ...
I0119 19:09:03.183489   15922 api_server.go:269] stopped: https://127.0.0.1:53549/healthz: Get "https://127.0.0.1:53549/healthz": EOF
I0119 19:09:03.680669   15922 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53549/healthz ...
I0119 19:09:07.221695   15922 api_server.go:279] https://127.0.0.1:53549/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0119 19:09:07.221740   15922 api_server.go:103] status: https://127.0.0.1:53549/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0119 19:09:07.221777   15922 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53549/healthz ...
I0119 19:09:07.339722   15922 api_server.go:279] https://127.0.0.1:53549/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0119 19:09:07.339751   15922 api_server.go:103] status: https://127.0.0.1:53549/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0119 19:09:07.680097   15922 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53549/healthz ...
I0119 19:09:07.689347   15922 api_server.go:279] https://127.0.0.1:53549/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0119 19:09:07.689423   15922 api_server.go:103] status: https://127.0.0.1:53549/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0119 19:09:08.179997   15922 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53549/healthz ...
I0119 19:09:08.227704   15922 api_server.go:279] https://127.0.0.1:53549/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0119 19:09:08.227747   15922 api_server.go:103] status: https://127.0.0.1:53549/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0119 19:09:08.679877   15922 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53549/healthz ...
I0119 19:09:08.725622   15922 api_server.go:279] https://127.0.0.1:53549/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0119 19:09:08.725693   15922 api_server.go:103] status: https://127.0.0.1:53549/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0119 19:09:09.179931   15922 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53549/healthz ...
I0119 19:09:09.219336   15922 api_server.go:279] https://127.0.0.1:53549/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0119 19:09:09.219364   15922 api_server.go:103] status: https://127.0.0.1:53549/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0119 19:09:09.679837   15922 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53549/healthz ...
I0119 19:09:09.711533   15922 api_server.go:279] https://127.0.0.1:53549/healthz returned 200:
ok
I0119 19:09:09.822369   15922 api_server.go:141] control plane version: v1.28.3
I0119 19:09:09.822383   15922 api_server.go:131] duration metric: took 7.658900837s to wait for apiserver health ...
I0119 19:09:09.822405   15922 cni.go:84] Creating CNI manager for ""
I0119 19:09:09.822431   15922 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0119 19:09:09.875150   15922 out.go:177] üîó  Configuring bridge CNI (Container Networking Interface) ...
I0119 19:09:09.937302   15922 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0119 19:09:09.966572   15922 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (457 bytes)
I0119 19:09:10.052105   15922 system_pods.go:43] waiting for kube-system pods to appear ...
I0119 19:09:10.265186   15922 system_pods.go:59] 7 kube-system pods found
I0119 19:09:10.265224   15922 system_pods.go:61] "coredns-5dd5756b68-r8hd2" [95f6adef-230f-4995-a672-44ea85ea3921] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0119 19:09:10.265231   15922 system_pods.go:61] "etcd-minikube" [e59a8a15-5532-4128-8245-9df5fd1d777f] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0119 19:09:10.265237   15922 system_pods.go:61] "kube-apiserver-minikube" [1b621f06-19c3-47bf-b9c6-28f5f674aebd] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0119 19:09:10.265243   15922 system_pods.go:61] "kube-controller-manager-minikube" [45d63589-20ff-4aec-910a-a65f35fe93e7] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0119 19:09:10.265262   15922 system_pods.go:61] "kube-proxy-7nt22" [55011390-4bfe-4fc1-9df8-2b5ae3edb3a2] Running
I0119 19:09:10.265270   15922 system_pods.go:61] "kube-scheduler-minikube" [e6d4680d-7f4e-4660-95e9-bdacef0b402a] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0119 19:09:10.265273   15922 system_pods.go:61] "storage-provisioner" [87af21ec-61ea-4391-b258-cd131dc68545] Running
I0119 19:09:10.265279   15922 system_pods.go:74] duration metric: took 213.154474ms to wait for pod list to return data ...
I0119 19:09:10.265284   15922 node_conditions.go:102] verifying NodePressure condition ...
I0119 19:09:10.322952   15922 node_conditions.go:122] node storage ephemeral capacity is 61202244Ki
I0119 19:09:10.322989   15922 node_conditions.go:123] node cpu capacity is 9
I0119 19:09:10.323868   15922 node_conditions.go:105] duration metric: took 58.574124ms to run NodePressure ...
I0119 19:09:10.326663   15922 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I0119 19:09:11.230092   15922 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0119 19:09:11.249759   15922 ops.go:34] apiserver oom_adj: -16
I0119 19:09:11.250082   15922 kubeadm.go:640] restartCluster took 24.636034122s
I0119 19:09:11.250124   15922 kubeadm.go:406] StartCluster complete in 24.685730127s
I0119 19:09:11.250395   15922 settings.go:142] acquiring lock: {Name:mkaedb835bc802856190cb3a4493ec1faeee4592 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0119 19:09:11.251881   15922 settings.go:150] Updating kubeconfig:  /Users/athikafatima/.kube/config
I0119 19:09:11.253215   15922 lock.go:35] WriteFile acquiring /Users/athikafatima/.kube/config: {Name:mk12c0c56e95c033a7ed2a0a9425636f9df0fdb9 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0119 19:09:11.254981   15922 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0119 19:09:11.255345   15922 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0119 19:09:11.255579   15922 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false]
I0119 19:09:11.255829   15922 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0119 19:09:11.255875   15922 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0119 19:09:11.256146   15922 addons.go:231] Setting addon storage-provisioner=true in "minikube"
W0119 19:09:11.256155   15922 addons.go:240] addon storage-provisioner should already be in state true
I0119 19:09:11.256345   15922 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0119 19:09:11.257480   15922 host.go:66] Checking if "minikube" exists ...
I0119 19:09:11.258136   15922 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0119 19:09:11.258372   15922 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0119 19:09:11.265058   15922 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0119 19:09:11.265191   15922 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I0119 19:09:11.285996   15922 out.go:177] üîé  Verifying Kubernetes components...
I0119 19:09:11.338823   15922 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0119 19:09:11.774964   15922 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0119 19:09:11.775620   15922 addons.go:231] Setting addon default-storageclass=true in "minikube"
W0119 19:09:11.782970   15922 addons.go:240] addon default-storageclass should already be in state true
I0119 19:09:11.783012   15922 host.go:66] Checking if "minikube" exists ...
I0119 19:09:11.783861   15922 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0119 19:09:11.783876   15922 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0119 19:09:11.784004   15922 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0119 19:09:11.784148   15922 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0119 19:09:11.919389   15922 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I0119 19:09:11.919418   15922 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0119 19:09:11.919531   15922 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0119 19:09:11.927194   15922 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53550 SSHKeyPath:/Users/athikafatima/.minikube/machines/minikube/id_rsa Username:docker}
I0119 19:09:12.034825   15922 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53550 SSHKeyPath:/Users/athikafatima/.minikube/machines/minikube/id_rsa Username:docker}
I0119 19:09:12.151011   15922 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0119 19:09:12.230993   15922 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0119 19:09:13.242384   15922 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml": (1.987036248s)
I0119 19:09:13.242411   15922 ssh_runner.go:235] Completed: sudo systemctl is-active --quiet service kubelet: (1.903599553s)
I0119 19:09:13.242492   15922 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0119 19:09:13.248131   15922 start.go:899] CoreDNS already contains "host.minikube.internal" host record, skipping...
I0119 19:09:13.347021   15922 api_server.go:52] waiting for apiserver process to appear ...
I0119 19:09:13.347162   15922 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0119 19:09:14.001097   15922 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.850084604s)
I0119 19:09:14.001197   15922 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (1.77020809s)
I0119 19:09:14.001229   15922 api_server.go:72] duration metric: took 2.735966442s to wait for apiserver process to appear ...
I0119 19:09:14.001235   15922 api_server.go:88] waiting for apiserver healthz status ...
I0119 19:09:14.001247   15922 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53549/healthz ...
I0119 19:09:14.010478   15922 api_server.go:279] https://127.0.0.1:53549/healthz returned 200:
ok
I0119 19:09:14.013269   15922 api_server.go:141] control plane version: v1.28.3
I0119 19:09:14.013282   15922 api_server.go:131] duration metric: took 12.043351ms to wait for apiserver health ...
I0119 19:09:14.013286   15922 system_pods.go:43] waiting for kube-system pods to appear ...
I0119 19:09:14.034638   15922 out.go:177] üåü  Enabled addons: storage-provisioner, default-storageclass
I0119 19:09:14.027685   15922 system_pods.go:59] 7 kube-system pods found
I0119 19:09:14.070449   15922 addons.go:502] enable addons completed in 2.81618012s: enabled=[storage-provisioner default-storageclass]
I0119 19:09:14.034687   15922 system_pods.go:61] "coredns-5dd5756b68-r8hd2" [95f6adef-230f-4995-a672-44ea85ea3921] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0119 19:09:14.070509   15922 system_pods.go:61] "etcd-minikube" [e59a8a15-5532-4128-8245-9df5fd1d777f] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0119 19:09:14.070548   15922 system_pods.go:61] "kube-apiserver-minikube" [1b621f06-19c3-47bf-b9c6-28f5f674aebd] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0119 19:09:14.070561   15922 system_pods.go:61] "kube-controller-manager-minikube" [45d63589-20ff-4aec-910a-a65f35fe93e7] Running
I0119 19:09:14.070573   15922 system_pods.go:61] "kube-proxy-7nt22" [55011390-4bfe-4fc1-9df8-2b5ae3edb3a2] Running
I0119 19:09:14.070579   15922 system_pods.go:61] "kube-scheduler-minikube" [e6d4680d-7f4e-4660-95e9-bdacef0b402a] Running
I0119 19:09:14.070584   15922 system_pods.go:61] "storage-provisioner" [87af21ec-61ea-4391-b258-cd131dc68545] Running
I0119 19:09:14.070590   15922 system_pods.go:74] duration metric: took 57.299789ms to wait for pod list to return data ...
I0119 19:09:14.070597   15922 kubeadm.go:581] duration metric: took 2.805337891s to wait for : map[apiserver:true system_pods:true] ...
I0119 19:09:14.070613   15922 node_conditions.go:102] verifying NodePressure condition ...
I0119 19:09:14.083459   15922 node_conditions.go:122] node storage ephemeral capacity is 61202244Ki
I0119 19:09:14.083475   15922 node_conditions.go:123] node cpu capacity is 9
I0119 19:09:14.083521   15922 node_conditions.go:105] duration metric: took 12.897471ms to run NodePressure ...
I0119 19:09:14.083534   15922 start.go:228] waiting for startup goroutines ...
I0119 19:09:14.083545   15922 start.go:233] waiting for cluster config update ...
I0119 19:09:14.083568   15922 start.go:242] writing updated cluster config ...
I0119 19:09:14.085098   15922 ssh_runner.go:195] Run: rm -f paused
I0119 19:09:14.171740   15922 start.go:600] kubectl: 1.27.2, cluster: 1.28.3 (minor skew: 1)
I0119 19:09:14.190493   15922 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* Jan 20 00:08:43 minikube cri-dockerd[1031]: time="2024-01-20T00:08:43Z" level=info msg="Start docker client with request timeout 0s"
Jan 20 00:08:43 minikube cri-dockerd[1031]: time="2024-01-20T00:08:43Z" level=info msg="Hairpin mode is set to hairpin-veth"
Jan 20 00:08:44 minikube cri-dockerd[1031]: time="2024-01-20T00:08:44Z" level=info msg="Loaded network plugin cni"
Jan 20 00:08:44 minikube cri-dockerd[1031]: time="2024-01-20T00:08:44Z" level=info msg="Docker cri networking managed by network plugin cni"
Jan 20 00:08:44 minikube cri-dockerd[1031]: time="2024-01-20T00:08:44Z" level=info msg="Docker Info: &{ID:f9a5b915-74df-43cc-8790-0b493d3fc819 Containers:15 ContainersRunning:0 ContainersPaused:0 ContainersStopped:15 Images:8 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:[] Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:[] Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6tables:true Debug:false NFd:24 OomKillDisable:false NGoroutines:35 SystemTime:2024-01-20T00:08:44.008448939Z LoggingDriver:json-file CgroupDriver:cgroupfs CgroupVersion:2 NEventsListener:0 KernelVersion:6.4.16-linuxkit OperatingSystem:Ubuntu 22.04.3 LTS OSVersion:22.04 OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:0xc0002404d0 NCPU:8 MemTotal:4120915968 GenericResources:[] DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy:control-plane.minikube.internal Name:minikube Labels:[provider=docker] ExperimentalBuild:false ServerVersion:24.0.7 ClusterStore: ClusterAdvertise: Runtimes:map[io.containerd.runc.v2:{Path:runc Args:[] Shim:<nil>} runc:{Path:runc Args:[] Shim:<nil>}] DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:[] Nodes:0 Managers:0 Cluster:<nil> Warnings:[]} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:61f9fd88f79f081d64d6fa3bb1a0dc71ec870523 Expected:61f9fd88f79f081d64d6fa3bb1a0dc71ec870523} RuncCommit:{ID:v1.1.9-0-gccaecfc Expected:v1.1.9-0-gccaecfc} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: DefaultAddressPools:[] Warnings:[]}"
Jan 20 00:08:44 minikube cri-dockerd[1031]: time="2024-01-20T00:08:44Z" level=info msg="Setting cgroupDriver cgroupfs"
Jan 20 00:08:44 minikube cri-dockerd[1031]: time="2024-01-20T00:08:44Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Jan 20 00:08:44 minikube cri-dockerd[1031]: time="2024-01-20T00:08:44Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Jan 20 00:08:44 minikube cri-dockerd[1031]: time="2024-01-20T00:08:44Z" level=info msg="Start cri-dockerd grpc backend"
Jan 20 00:08:44 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Jan 20 00:08:59 minikube cri-dockerd[1031]: time="2024-01-20T00:08:59Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5dd5756b68-r8hd2_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"6eb1ba2a24ba698b6d8556d2f72db559c3c00328878c0fa7790be931c6ffb8e1\""
Jan 20 00:09:01 minikube cri-dockerd[1031]: time="2024-01-20T00:09:01Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1b69576985356c0f55f39e84885e819b885ba6e2239090056d2d048976c8d1f5/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jan 20 00:09:01 minikube cri-dockerd[1031]: time="2024-01-20T00:09:01Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a640ecb5b6c4ca624e10394a27cad30959ae439578f357d5f8e41bdddc95636d/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jan 20 00:09:01 minikube cri-dockerd[1031]: time="2024-01-20T00:09:01Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/891af1ac955b315a87a0f88db7516301881ef97996e4ea57f5774febb37227dd/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jan 20 00:09:01 minikube cri-dockerd[1031]: time="2024-01-20T00:09:01Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3da5e8666ca7092bd7a9a6eac154ebe7c15985582efee5f247c4898ce341eaa4/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jan 20 00:09:07 minikube cri-dockerd[1031]: time="2024-01-20T00:09:07Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Jan 20 00:09:08 minikube cri-dockerd[1031]: time="2024-01-20T00:09:08Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9d00d3e0d5b07571e51d7947211390331aed876a7ce20bf30c5e3ea02be95225/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jan 20 00:09:08 minikube cri-dockerd[1031]: time="2024-01-20T00:09:08Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4b29fc8beba176fa44a52e5435729122ecf0ba6532d814a008dc2db01429841a/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jan 20 00:09:09 minikube cri-dockerd[1031]: time="2024-01-20T00:09:09Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/edf805ea2e122bca460d0e93dcc35849108603677bdc640d612d18504afafc84/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jan 20 00:09:41 minikube dockerd[802]: time="2024-01-20T00:09:41.087120915Z" level=info msg="ignoring event" container=ec4980a1cf947f8802160e4a14dffe944969578d56f5d404fcc36e815e66a3c7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 20 00:10:16 minikube cri-dockerd[1031]: time="2024-01-20T00:10:16Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/cbf47d6664cd0500b38eac58a672b964f71b1281a142b0925cf2f6dd3425d508/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 20 00:10:23 minikube cri-dockerd[1031]: time="2024-01-20T00:10:23Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5625bc39e4f10cff1589cd9d82447419e773f59507bb82787958c5f76d89130a/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 20 00:10:23 minikube cri-dockerd[1031]: time="2024-01-20T00:10:23Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/85ad18e960b9f92c77a7b0556698346598c42fa22437e59b906011953526e450/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 20 00:10:27 minikube cri-dockerd[1031]: time="2024-01-20T00:10:27Z" level=info msg="Pulling image mongo:latest: 06fc900f7e64: Downloading [==================>                                ]  82.89MB/224.4MB"
Jan 20 00:10:37 minikube cri-dockerd[1031]: time="2024-01-20T00:10:37Z" level=info msg="Pulling image mongo:latest: 06fc900f7e64: Downloading [=================================================> ]  222.8MB/224.4MB"
Jan 20 00:10:47 minikube cri-dockerd[1031]: time="2024-01-20T00:10:47Z" level=info msg="Pulling image mongo:latest: 06fc900f7e64: Extracting [==========================>                        ]  118.1MB/224.4MB"
Jan 20 00:10:57 minikube cri-dockerd[1031]: time="2024-01-20T00:10:57Z" level=info msg="Pulling image mongo:latest: 06fc900f7e64: Extracting [==========================================>        ]    190MB/224.4MB"
Jan 20 00:11:01 minikube cri-dockerd[1031]: time="2024-01-20T00:11:01Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8d0be2dfef1ec2258c571a2627033c6140ae47357f2d0b1acff0f04ed34f2911/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 20 00:11:01 minikube cri-dockerd[1031]: time="2024-01-20T00:11:01Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3385c537df1babfe38de2fa64098bc4933bfad4a563250564c0b3fc2ed89777f/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 20 00:11:07 minikube cri-dockerd[1031]: time="2024-01-20T00:11:07Z" level=info msg="Pulling image mongo:latest: 06fc900f7e64: Pull complete "
Jan 20 00:11:07 minikube cri-dockerd[1031]: time="2024-01-20T00:11:07Z" level=info msg="Stop pulling image mongo:latest: Status: Downloaded newer image for mongo:latest"
Jan 20 00:11:19 minikube cri-dockerd[1031]: time="2024-01-20T00:11:19Z" level=info msg="Pulling image ghcr.io/ananya2001-an/note-server:latest: debce5f9f3a9: Downloading [===============================================>   ]  60.54MB/64.11MB"
Jan 20 00:11:19 minikube cri-dockerd[1031]: time="2024-01-20T00:11:19Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3025a50ed04b29f8c82464d648251c20248518ab32e548d5ab5d7c6daf9664e1/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 20 00:11:29 minikube cri-dockerd[1031]: time="2024-01-20T00:11:29Z" level=info msg="Pulling image ghcr.io/ananya2001-an/note-server:latest: 1d7ca7cd2e06: Downloading [=====================>                             ]  88.65MB/211MB"
Jan 20 00:11:39 minikube cri-dockerd[1031]: time="2024-01-20T00:11:39Z" level=info msg="Pulling image ghcr.io/ananya2001-an/note-server:latest: b47a222d28fa: Extracting [===========================================>       ]  20.97MB/24.03MB"
Jan 20 00:11:49 minikube cri-dockerd[1031]: time="2024-01-20T00:11:49Z" level=info msg="Pulling image ghcr.io/ananya2001-an/note-server:latest: 1d7ca7cd2e06: Extracting [=>                                                 ]  5.014MB/211MB"
Jan 20 00:11:59 minikube cri-dockerd[1031]: time="2024-01-20T00:11:59Z" level=info msg="Pulling image ghcr.io/ananya2001-an/note-server:latest: 1d7ca7cd2e06: Extracting [======>                                            ]  27.85MB/211MB"
Jan 20 00:12:09 minikube cri-dockerd[1031]: time="2024-01-20T00:12:09Z" level=info msg="Pulling image ghcr.io/ananya2001-an/note-server:latest: 1d7ca7cd2e06: Extracting [======================>                            ]  96.93MB/211MB"
Jan 20 00:12:19 minikube cri-dockerd[1031]: time="2024-01-20T00:12:19Z" level=info msg="Pulling image ghcr.io/ananya2001-an/note-server:latest: 1d7ca7cd2e06: Extracting [=================================>                 ]  143.2MB/211MB"
Jan 20 00:12:24 minikube cri-dockerd[1031]: time="2024-01-20T00:12:24Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f034fb02521dce477893025ae0ea686406e0ac9dde6805a4f82eba7a8cebda31/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 20 00:12:29 minikube cri-dockerd[1031]: time="2024-01-20T00:12:29Z" level=info msg="Pulling image ghcr.io/ananya2001-an/note-server:latest: 1d7ca7cd2e06: Extracting [=================================================> ]  207.8MB/211MB"
Jan 20 00:12:39 minikube cri-dockerd[1031]: time="2024-01-20T00:12:39Z" level=info msg="Pulling image ghcr.io/ananya2001-an/note-server:latest: c4aa25333915: Extracting [===========================>                       ]  25.56MB/45.97MB"
Jan 20 00:12:49 minikube cri-dockerd[1031]: time="2024-01-20T00:12:49Z" level=info msg="Pulling image ghcr.io/ananya2001-an/note-server:latest: a3f7ec7b07ae: Extracting [==================================================>]  9.798MB/9.798MB"
Jan 20 00:12:50 minikube cri-dockerd[1031]: time="2024-01-20T00:12:50Z" level=info msg="Stop pulling image ghcr.io/ananya2001-an/note-server:latest: Status: Downloaded newer image for ghcr.io/ananya2001-an/note-server:latest"
Jan 20 00:12:53 minikube cri-dockerd[1031]: time="2024-01-20T00:12:53Z" level=info msg="Stop pulling image ghcr.io/ananya2001-an/note-server:latest: Status: Image is up to date for ghcr.io/ananya2001-an/note-server:latest"
Jan 20 00:13:04 minikube cri-dockerd[1031]: time="2024-01-20T00:13:04Z" level=info msg="Pulling image ghcr.io/ananya2001-an/note:latest: 708173787fc8: Extracting [==================================================>]  1.799MB/1.799MB"
Jan 20 00:13:09 minikube cri-dockerd[1031]: time="2024-01-20T00:13:09Z" level=info msg="Stop pulling image ghcr.io/ananya2001-an/note:latest: Status: Downloaded newer image for ghcr.io/ananya2001-an/note:latest"
Jan 20 00:13:10 minikube cri-dockerd[1031]: time="2024-01-20T00:13:10Z" level=info msg="Stop pulling image ghcr.io/ananya2001-an/note:latest: Status: Image is up to date for ghcr.io/ananya2001-an/note:latest"
Jan 20 00:13:11 minikube cri-dockerd[1031]: time="2024-01-20T00:13:11Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Jan 20 00:13:21 minikube cri-dockerd[1031]: time="2024-01-20T00:13:21Z" level=info msg="Pulling image mongo-express:latest: bc7465dc4da3: Extracting [==========>                                        ]   8.52MB/40.11MB"
Jan 20 00:13:31 minikube cri-dockerd[1031]: time="2024-01-20T00:13:31Z" level=info msg="Pulling image mongo-express:latest: bc7465dc4da3: Extracting [=============================================>     ]  36.63MB/40.11MB"
Jan 20 00:13:36 minikube dockerd[802]: time="2024-01-20T00:13:36.731915843Z" level=info msg="ignoring event" container=b135140aef57692bdd92cbe7cae1eaa8c4eb784e8e14a0c2d5fd42238c737161 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 20 00:13:41 minikube cri-dockerd[1031]: time="2024-01-20T00:13:41Z" level=info msg="Pulling image mongo-express:latest: bc7465dc4da3: Extracting [================================================>  ]  38.76MB/40.11MB"
Jan 20 00:13:51 minikube cri-dockerd[1031]: time="2024-01-20T00:13:51Z" level=info msg="Pulling image mongo-express:latest: 76bb6e5f1580: Extracting [=========>                                         ]  6.128MB/33.06MB"
Jan 20 00:14:01 minikube cri-dockerd[1031]: time="2024-01-20T00:14:01Z" level=info msg="Pulling image mongo-express:latest: 76bb6e5f1580: Extracting [==================>                                ]  12.26MB/33.06MB"
Jan 20 00:14:11 minikube cri-dockerd[1031]: time="2024-01-20T00:14:11Z" level=info msg="Pulling image mongo-express:latest: 76bb6e5f1580: Extracting [=======================>                           ]  15.86MB/33.06MB"
Jan 20 00:14:21 minikube cri-dockerd[1031]: time="2024-01-20T00:14:21Z" level=info msg="Pulling image mongo-express:latest: 76bb6e5f1580: Extracting [===============================>                   ]  20.91MB/33.06MB"
Jan 20 00:14:31 minikube cri-dockerd[1031]: time="2024-01-20T00:14:31Z" level=info msg="Pulling image mongo-express:latest: 76bb6e5f1580: Extracting [=======================================>           ]  26.31MB/33.06MB"
Jan 20 00:14:41 minikube cri-dockerd[1031]: time="2024-01-20T00:14:41Z" level=info msg="Pulling image mongo-express:latest: 76bb6e5f1580: Extracting [============================================>      ]  29.56MB/33.06MB"
Jan 20 00:14:46 minikube cri-dockerd[1031]: time="2024-01-20T00:14:46Z" level=info msg="Stop pulling image mongo-express:latest: Status: Downloaded newer image for mongo-express:latest"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                                       CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
b8cbfc345c555       mongo-express@sha256:98ad29b0d7cbeeba8f4640cdcd11e097081ed0ca27c89b4ee0cde63e3bfe53f1                       2 minutes ago       Running             mongo-express             0                   f034fb02521dc       mongo-express-deployment-b88f6d45f-k2hkj
81b2e61bc8fd9       6e38f40d628db                                                                                               3 minutes ago       Running             storage-provisioner       5                   9d00d3e0d5b07       storage-provisioner
e3c77849cbbff       mongo@sha256:192e2724093257a7db12db6cbafd92e3e5d51937f13846d49ea555cea85787ce                               4 minutes ago       Running             mongodb                   0                   3025a50ed04b2       mongodb-stateful-set-1
a13943551ee16       ghcr.io/ananya2001-an/note@sha256:b3b28a9946bccbe339ea598cb49629bd7f9af13004c9d90197e2728b20e6bfc2          4 minutes ago       Running             note                      0                   3385c537df1ba       note-deployment-74cc946cd8-kxxtx
59320d38271db       ghcr.io/ananya2001-an/note@sha256:b3b28a9946bccbe339ea598cb49629bd7f9af13004c9d90197e2728b20e6bfc2          4 minutes ago       Running             note                      0                   8d0be2dfef1ec       note-deployment-74cc946cd8-x6smv
72263ea1335a5       ghcr.io/ananya2001-an/note-server@sha256:d6db681ce86fffc3714bee097212ff69c64521f0acf4f76ed53245b9d60d44a3   4 minutes ago       Running             note-server               0                   5625bc39e4f10       note-server-deployment-6fb5fcb67f-nvfsp
ee3e46d3f4299       ghcr.io/ananya2001-an/note-server@sha256:d6db681ce86fffc3714bee097212ff69c64521f0acf4f76ed53245b9d60d44a3   4 minutes ago       Running             note-server               0                   85ad18e960b9f       note-server-deployment-6fb5fcb67f-fln28
5d3e4f41093e2       mongo@sha256:192e2724093257a7db12db6cbafd92e3e5d51937f13846d49ea555cea85787ce                               6 minutes ago       Running             mongodb                   0                   cbf47d6664cd0       mongodb-stateful-set-0
b135140aef576       6e38f40d628db                                                                                               7 minutes ago       Exited              storage-provisioner       4                   9d00d3e0d5b07       storage-provisioner
d37086634e921       ead0a4a53df89                                                                                               8 minutes ago       Running             coredns                   1                   edf805ea2e122       coredns-5dd5756b68-r8hd2
ca8b5ab9240b7       bfc896cf80fba                                                                                               8 minutes ago       Running             kube-proxy                1                   4b29fc8beba17       kube-proxy-7nt22
dcfb1395d3992       10baa1ca17068                                                                                               8 minutes ago       Running             kube-controller-manager   1                   891af1ac955b3       kube-controller-manager-minikube
a5c7f3ee624b5       6d1b4fd1b182d                                                                                               8 minutes ago       Running             kube-scheduler            1                   3da5e8666ca70       kube-scheduler-minikube
1796c8064f635       5374347291230                                                                                               8 minutes ago       Running             kube-apiserver            1                   1b69576985356       kube-apiserver-minikube
492ff0e4b46c2       73deb9a3f7025                                                                                               8 minutes ago       Running             etcd                      1                   a640ecb5b6c4c       etcd-minikube
2cae5dd92509c       ead0a4a53df89                                                                                               21 hours ago        Exited              coredns                   0                   6eb1ba2a24ba6       coredns-5dd5756b68-r8hd2
6f0ca9033a94c       bfc896cf80fba                                                                                               21 hours ago        Exited              kube-proxy                0                   130ce2c4a90e8       kube-proxy-7nt22
b1f1cf24b4c70       5374347291230                                                                                               21 hours ago        Exited              kube-apiserver            0                   c11d5b7c1c021       kube-apiserver-minikube
fbcaf7940993a       10baa1ca17068                                                                                               21 hours ago        Exited              kube-controller-manager   0                   b7b652350db62       kube-controller-manager-minikube
bf65849b83053       73deb9a3f7025                                                                                               21 hours ago        Exited              etcd                      0                   d6e725046f442       etcd-minikube
28d5cd0baa7a5       6d1b4fd1b182d                                                                                               21 hours ago        Exited              kube-scheduler            0                   e50648fc1c4c3       kube-scheduler-minikube

* 
* ==> coredns [2cae5dd92509] <==
* .:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:46841 - 12534 "HINFO IN 6253976467554960601.1996739617877033437. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.052377189s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.670768569s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.73540796s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.435037216s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.25336109s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.768261532s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.180267211s

* 
* ==> coredns [d37086634e92] <==
* [INFO] 10.244.0.10:48415 - 32818 "A IN mongo. udp 23 false 512" NXDOMAIN qr,rd,ra 23 0.01895697s
[INFO] 10.244.0.10:41238 - 55381 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000147864s
[INFO] 10.244.0.10:41238 - 55001 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.00041027s
[INFO] 10.244.0.10:54372 - 19412 "A IN mongo.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.00022101s
[INFO] 10.244.0.10:54372 - 19629 "AAAA IN mongo.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000451855s
[INFO] 10.244.0.10:44403 - 6125 "AAAA IN mongo.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.000206434s
[INFO] 10.244.0.10:44403 - 5831 "A IN mongo.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.000177095s
[INFO] 10.244.0.10:59404 - 14555 "A IN mongo. udp 23 false 512" NXDOMAIN qr,rd,ra 23 0.009167947s
[INFO] 10.244.0.10:59404 - 14833 "AAAA IN mongo. udp 23 false 512" NXDOMAIN qr,rd,ra 23 0.019732379s
[INFO] 10.244.0.10:47765 - 34141 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000301249s
[INFO] 10.244.0.10:49117 - 39999 "AAAA IN mongo.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000208319s
[INFO] 10.244.0.10:49117 - 39715 "A IN mongo.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000274819s
[INFO] 10.244.0.10:43576 - 2148 "AAAA IN mongo.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.0001682s
[INFO] 10.244.0.10:43576 - 1807 "A IN mongo.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.000239872s
[INFO] 10.244.0.10:47765 - 33872 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.00055071s
[INFO] 10.244.0.10:43597 - 40282 "A IN mongo. udp 23 false 512" NXDOMAIN qr,rd,ra 23 0.011543708s
[INFO] 10.244.0.10:43597 - 40583 "AAAA IN mongo. udp 23 false 512" NXDOMAIN qr,rd,ra 23 0.012052461s
[INFO] 10.244.0.10:52611 - 49006 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000207054s
[INFO] 10.244.0.10:52611 - 48690 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000625366s
[INFO] 10.244.0.10:52184 - 25369 "A IN mongo.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000323916s
[INFO] 10.244.0.10:57038 - 6359 "AAAA IN mongo.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.000082522s
[INFO] 10.244.0.10:57038 - 6050 "A IN mongo.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.000303551s
[INFO] 10.244.0.10:52184 - 25708 "AAAA IN mongo.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000538747s
[INFO] 10.244.0.10:41890 - 6180 "A IN mongo. udp 23 false 512" NXDOMAIN qr,rd,ra 23 0.013248046s
[INFO] 10.244.0.10:41890 - 6471 "AAAA IN mongo. udp 23 false 512" NXDOMAIN qr,rd,ra 23 0.016607036s
[INFO] 10.244.0.10:42714 - 16937 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.00045446s
[INFO] 10.244.0.10:42714 - 16588 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000755312s
[INFO] 10.244.0.10:46045 - 40428 "AAAA IN mongo.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000148937s
[INFO] 10.244.0.10:46045 - 40135 "A IN mongo.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000186426s
[INFO] 10.244.0.10:51735 - 10148 "AAAA IN mongo.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.000211053s
[INFO] 10.244.0.10:51735 - 9829 "A IN mongo.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.00013282s
[INFO] 10.244.0.10:60139 - 30802 "A IN mongo. udp 23 false 512" NXDOMAIN qr,rd,ra 23 0.075806833s
[INFO] 10.244.0.10:60139 - 31095 "AAAA IN mongo. udp 23 false 512" NXDOMAIN qr,rd,ra 23 0.075951602s
[INFO] 10.244.0.10:36214 - 32172 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000235242s
[INFO] 10.244.0.10:36214 - 31853 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000481497s
[INFO] 10.244.0.10:49576 - 47374 "AAAA IN mongo.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000148212s
[INFO] 10.244.0.10:49576 - 47002 "A IN mongo.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.00015972s
[INFO] 10.244.0.10:40087 - 4140 "A IN mongo.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.000191051s
[INFO] 10.244.0.10:40087 - 4487 "AAAA IN mongo.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.000801584s
[INFO] 10.244.0.10:35420 - 23130 "AAAA IN mongo. udp 23 false 512" NXDOMAIN qr,rd,ra 23 0.012037901s
[INFO] 10.244.0.10:35420 - 22811 "A IN mongo. udp 23 false 512" NXDOMAIN qr,rd,ra 23 0.01581122s
[INFO] 10.244.0.10:35346 - 8474 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000197164s
[INFO] 10.244.0.10:35346 - 8214 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000453688s
[INFO] 10.244.0.10:60979 - 50199 "AAAA IN mongo.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000266066s
[INFO] 10.244.0.10:60979 - 49853 "A IN mongo.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000404665s
[INFO] 10.244.0.10:58470 - 27698 "AAAA IN mongo.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.000121641s
[INFO] 10.244.0.10:58470 - 27428 "A IN mongo.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.000219299s
[INFO] 10.244.0.10:44175 - 26922 "A IN mongo. udp 23 false 512" NXDOMAIN qr,rd,ra 23 0.013233552s
[INFO] 10.244.0.10:44175 - 27379 "AAAA IN mongo. udp 23 false 512" NXDOMAIN qr,rd,ra 23 0.013566866s
[INFO] 10.244.0.10:33230 - 24880 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000201066s
[INFO] 10.244.0.10:33230 - 24412 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000312304s
[INFO] 10.244.0.10:41737 - 63766 "AAAA IN mongo.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000155937s
[INFO] 10.244.0.10:41737 - 63289 "A IN mongo.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000115022s
[INFO] 10.244.0.10:33952 - 19504 "AAAA IN mongo.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.000353244s
[INFO] 10.244.0.10:33952 - 19143 "A IN mongo.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.000690019s
[INFO] 10.244.0.10:44050 - 883 "AAAA IN mongo. udp 23 false 512" NXDOMAIN qr,rd,ra 23 0.011198905s
[INFO] 10.244.0.10:44050 - 498 "A IN mongo. udp 23 false 512" NXDOMAIN qr,rd,ra 23 0.014772625s
[INFO] 10.244.0.10:51563 - 8248 "A IN mongodb-service.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.000309738s
[INFO] 10.244.0.10:45839 - 47055 "A IN mongodb-service.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.000142156s
[INFO] 10.244.0.10:56696 - 62212 "A IN mongodb-service.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.000257655s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=8220a6eb95f0a4d75f7f2d7b14cef975f050512d
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_01_18T22_09_35_0700
                    minikube.k8s.io/version=v1.32.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Fri, 19 Jan 2024 03:09:31 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sat, 20 Jan 2024 00:17:41 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sat, 20 Jan 2024 00:15:11 +0000   Fri, 19 Jan 2024 03:29:24 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sat, 20 Jan 2024 00:15:11 +0000   Fri, 19 Jan 2024 03:29:24 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sat, 20 Jan 2024 00:15:11 +0000   Fri, 19 Jan 2024 03:29:24 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sat, 20 Jan 2024 00:15:11 +0000   Fri, 19 Jan 2024 04:21:11 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.58.2
  Hostname:    minikube
Capacity:
  cpu:                9
  ephemeral-storage:  61202244Ki
  hugepages-2Mi:      0
  memory:             4024332Ki
  pods:               110
Allocatable:
  cpu:                9
  ephemeral-storage:  61202244Ki
  hugepages-2Mi:      0
  memory:             4024332Ki
  pods:               110
System Info:
  Machine ID:                 93779dd1921c4bc995247e84718c0921
  System UUID:                93779dd1921c4bc995247e84718c0921
  Boot ID:                    74aca3ce-99ec-40ce-bbcc-40a5b5648028
  Kernel Version:             6.4.16-linuxkit
  OS Image:                   Ubuntu 22.04.3 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.7
  Kubelet Version:            v1.28.3
  Kube-Proxy Version:         v1.28.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (14 in total)
  Namespace                   Name                                        CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                        ------------  ----------  ---------------  -------------  ---
  default                     mongo-express-deployment-b88f6d45f-k2hkj    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         5m25s
  default                     mongodb-stateful-set-0                      0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         7m35s
  default                     mongodb-stateful-set-1                      0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         6m33s
  default                     note-deployment-74cc946cd8-kxxtx            0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         6m52s
  default                     note-deployment-74cc946cd8-x6smv            0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         6m52s
  default                     note-server-deployment-6fb5fcb67f-fln28     0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         7m26s
  default                     note-server-deployment-6fb5fcb67f-nvfsp     0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         7m26s
  kube-system                 coredns-5dd5756b68-r8hd2                    100m (1%!)(MISSING)     0 (0%!)(MISSING)      70Mi (1%!)(MISSING)        170Mi (4%!)(MISSING)     21h
  kube-system                 etcd-minikube                               100m (1%!)(MISSING)     0 (0%!)(MISSING)      100Mi (2%!)(MISSING)       0 (0%!)(MISSING)         21h
  kube-system                 kube-apiserver-minikube                     250m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         21h
  kube-system                 kube-controller-manager-minikube            200m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         21h
  kube-system                 kube-proxy-7nt22                            0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         21h
  kube-system                 kube-scheduler-minikube                     100m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         21h
  kube-system                 storage-provisioner                         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         21h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (8%!)(MISSING)   0 (0%!)(MISSING)
  memory             170Mi (4%!)(MISSING)  170Mi (4%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                    From             Message
  ----    ------                   ----                   ----             -------
  Normal  Starting                 8m34s                  kube-proxy       
  Normal  NodeNotReady             20h (x2 over 20h)      node-controller  Node minikube status is now: NodeNotReady
  Normal  NodeHasSufficientMemory  20h (x3 over 21h)      kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    20h (x3 over 21h)      kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     20h (x3 over 21h)      kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeNotReady             19h (x3 over 21h)      kubelet          Node minikube status is now: NodeNotReady
  Normal  NodeReady                19h (x4 over 21h)      kubelet          Node minikube status is now: NodeReady
  Normal  Starting                 8m47s                  kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  8m47s                  kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  8m46s (x8 over 8m47s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    8m46s (x8 over 8m47s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     8m46s (x7 over 8m47s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  RegisteredNode           8m25s                  node-controller  Node minikube event: Registered Node minikube in Controller

* 
* ==> dmesg <==
* [Jan20 00:08] PCI: Fatal: No config space access function found
[  +0.090061] pci 0000:00:1f.0: BAR 13: [io  size 0x0080] has bogus alignment
[  +0.031360] virtio-pci 0000:00:01.0: can't derive routing for PCI INT A
[  +0.000002] virtio-pci 0000:00:01.0: PCI INT A: no GSI
[  +0.003563] virtio-pci 0000:00:05.0: can't derive routing for PCI INT A
[  +0.000002] virtio-pci 0000:00:05.0: PCI INT A: no GSI
[  +0.002967] virtio-pci 0000:00:06.0: can't derive routing for PCI INT A
[  +0.000001] virtio-pci 0000:00:06.0: PCI INT A: no GSI
[  +0.002952] virtio-pci 0000:00:07.0: can't derive routing for PCI INT A
[  +0.000001] virtio-pci 0000:00:07.0: PCI INT A: no GSI
[  +0.004780] virtio-pci 0000:00:08.0: can't derive routing for PCI INT A
[  +0.000002] virtio-pci 0000:00:08.0: PCI INT A: no GSI
[  +0.005130] virtio-pci 0000:00:09.0: can't derive routing for PCI INT A
[  +0.000001] virtio-pci 0000:00:09.0: PCI INT A: no GSI
[  +0.004839] virtio-pci 0000:00:0a.0: can't derive routing for PCI INT A
[  +0.000001] virtio-pci 0000:00:0a.0: PCI INT A: no GSI
[  +0.005037] virtio-pci 0000:00:0b.0: can't derive routing for PCI INT A
[  +0.000000] virtio-pci 0000:00:0b.0: PCI INT A: no GSI
[  +0.005116] virtio-pci 0000:00:0c.0: can't derive routing for PCI INT A
[  +0.000002] virtio-pci 0000:00:0c.0: PCI INT A: no GSI
[  +0.000612] virtio-pci 0000:00:0d.0: can't derive routing for PCI INT A
[  +0.000001] virtio-pci 0000:00:0d.0: PCI INT A: no GSI
[  +0.000565] virtio-pci 0000:00:0e.0: can't derive routing for PCI INT A
[  +0.000001] virtio-pci 0000:00:0e.0: PCI INT A: no GSI
[  +0.009992] Hangcheck: starting hangcheck timer 0.9.1 (tick is 180 seconds, margin is 60 seconds).
[  +0.031025] lpc_ich 0000:00:1f.0: No MFD cells added
[  +0.007343] fail to initialize ptp_kvm
[  +0.000001] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +2.742219] 3[326]: memfd_create() called without MFD_EXEC or MFD_NOEXEC_SEAL set
[  +0.244059] FAT-fs (loop0): utf8 is not a recommended IO charset for FAT filesystems, filesystem will be case sensitive!
[  +0.000596] FAT-fs (loop0): utf8 is not a recommended IO charset for FAT filesystems, filesystem will be case sensitive!
[  +0.026968] grpcfuse: loading out-of-tree module taints kernel.
[Jan20 00:10] hrtimer: interrupt took 3505272 ns

* 
* ==> etcd [492ff0e4b46c] <==
* {"level":"info","ts":"2024-01-20T00:13:34.625141Z","caller":"traceutil/trace.go:171","msg":"trace[1673253108] transaction","detail":"{read_only:false; response_revision:1204; number_of_response:1; }","duration":"5.237282275s","start":"2024-01-20T00:13:29.65086Z","end":"2024-01-20T00:13:34.625129Z","steps":["trace[1673253108] 'process raft request'  (duration: 5.236585137s)"],"step_count":1}
{"level":"warn","ts":"2024-01-20T00:13:34.625305Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-20T00:13:29.650804Z","time spent":"5.237374989s","remote":"127.0.0.1:52000","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":673,"response count":0,"response size":39,"request content":"compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:1198 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:600 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >"}
{"level":"warn","ts":"2024-01-20T00:13:34.626743Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.307827214s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/csidrivers/\" range_end:\"/registry/csidrivers0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-01-20T00:13:34.626807Z","caller":"traceutil/trace.go:171","msg":"trace[536014665] range","detail":"{range_begin:/registry/csidrivers/; range_end:/registry/csidrivers0; response_count:0; response_revision:1204; }","duration":"1.307942673s","start":"2024-01-20T00:13:33.581863Z","end":"2024-01-20T00:13:34.626793Z","steps":["trace[536014665] 'agreement among raft nodes before linearized reading'  (duration: 1.3077652s)"],"step_count":1}
{"level":"warn","ts":"2024-01-20T00:13:34.626859Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-20T00:13:33.581833Z","time spent":"1.308024607s","remote":"127.0.0.1:52122","response type":"/etcdserverpb.KV/Range","request count":0,"request size":48,"response count":0,"response size":28,"request content":"key:\"/registry/csidrivers/\" range_end:\"/registry/csidrivers0\" count_only:true "}
{"level":"warn","ts":"2024-01-20T00:13:34.627282Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"4.890531083s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/192.168.58.2\" ","response":"range_response_count:1 size:131"}
{"level":"info","ts":"2024-01-20T00:13:34.627326Z","caller":"traceutil/trace.go:171","msg":"trace[1255144801] range","detail":"{range_begin:/registry/masterleases/192.168.58.2; range_end:; response_count:1; response_revision:1204; }","duration":"4.890580259s","start":"2024-01-20T00:13:29.999746Z","end":"2024-01-20T00:13:34.627313Z","steps":["trace[1255144801] 'agreement among raft nodes before linearized reading'  (duration: 4.890306323s)"],"step_count":1}
{"level":"warn","ts":"2024-01-20T00:13:34.62736Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-20T00:13:29.999729Z","time spent":"4.890635497s","remote":"127.0.0.1:51792","response type":"/etcdserverpb.KV/Range","request count":0,"request size":37,"response count":1,"response size":154,"request content":"key:\"/registry/masterleases/192.168.58.2\" "}
{"level":"warn","ts":"2024-01-20T00:13:34.627584Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"350.52494ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/controllers/\" range_end:\"/registry/controllers0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-01-20T00:13:34.627639Z","caller":"traceutil/trace.go:171","msg":"trace[2136293779] range","detail":"{range_begin:/registry/controllers/; range_end:/registry/controllers0; response_count:0; response_revision:1204; }","duration":"350.585651ms","start":"2024-01-20T00:13:34.277042Z","end":"2024-01-20T00:13:34.627628Z","steps":["trace[2136293779] 'agreement among raft nodes before linearized reading'  (duration: 350.47108ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-20T00:13:34.627705Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-20T00:13:34.277026Z","time spent":"350.665346ms","remote":"127.0.0.1:51940","response type":"/etcdserverpb.KV/Range","request count":0,"request size":50,"response count":0,"response size":28,"request content":"key:\"/registry/controllers/\" range_end:\"/registry/controllers0\" count_only:true "}
{"level":"warn","ts":"2024-01-20T00:13:35.010402Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"199.744548ms","expected-duration":"100ms","prefix":"","request":"header:<ID:3238524793671030395 username:\"kube-apiserver-etcd-client\" auth_revision:1 > lease_grant:<ttl:15-second id:2cf18d24330e967a>","response":"size:40"}
{"level":"info","ts":"2024-01-20T00:13:35.010529Z","caller":"traceutil/trace.go:171","msg":"trace[690069663] linearizableReadLoop","detail":"{readStateIndex:1379; appliedIndex:1378; }","duration":"275.938683ms","start":"2024-01-20T00:13:34.734573Z","end":"2024-01-20T00:13:35.010511Z","steps":["trace[690069663] 'read index received'  (duration: 75.207746ms)","trace[690069663] 'applied index is now lower than readState.Index'  (duration: 200.728638ms)"],"step_count":2}
{"level":"warn","ts":"2024-01-20T00:13:35.019433Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-20T00:13:34.636398Z","time spent":"383.030741ms","remote":"127.0.0.1:51792","response type":"/etcdserverpb.Lease/LeaseGrant","request count":-1,"request size":-1,"response count":-1,"response size":-1,"request content":""}
{"level":"warn","ts":"2024-01-20T00:13:35.417555Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"302.389676ms","expected-duration":"100ms","prefix":"","request":"header:<ID:3238524793671030398 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:1200 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >>","response":"size:16"}
{"level":"info","ts":"2024-01-20T00:13:35.418681Z","caller":"traceutil/trace.go:171","msg":"trace[791874476] transaction","detail":"{read_only:false; response_revision:1205; number_of_response:1; }","duration":"401.30513ms","start":"2024-01-20T00:13:35.017169Z","end":"2024-01-20T00:13:35.418474Z","steps":["trace[791874476] 'process raft request'  (duration: 93.19534ms)","trace[791874476] 'compare'  (duration: 197.27794ms)"],"step_count":2}
{"level":"warn","ts":"2024-01-20T00:13:35.419051Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-20T00:13:35.017117Z","time spent":"401.812432ms","remote":"127.0.0.1:52000","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":521,"response count":0,"response size":39,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:1200 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}
{"level":"warn","ts":"2024-01-20T00:13:35.435874Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"575.149114ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-01-20T00:13:35.436063Z","caller":"traceutil/trace.go:171","msg":"trace[180418008] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:1204; }","duration":"701.477875ms","start":"2024-01-20T00:13:34.734533Z","end":"2024-01-20T00:13:35.43601Z","steps":["trace[180418008] 'agreement among raft nodes before linearized reading'  (duration: 277.70585ms)","trace[180418008] 'range keys from in-memory index tree'  (duration: 297.136596ms)"],"step_count":2}
{"level":"warn","ts":"2024-01-20T00:13:35.436261Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-20T00:13:34.733165Z","time spent":"703.048114ms","remote":"127.0.0.1:51778","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":28,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2024-01-20T00:13:35.519218Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-20T00:13:35.019215Z","time spent":"499.997981ms","remote":"127.0.0.1:51828","response type":"/etcdserverpb.Lease/LeaseGrant","request count":-1,"request size":-1,"response count":-1,"response size":-1,"request content":""}
{"level":"info","ts":"2024-01-20T00:13:35.522015Z","caller":"traceutil/trace.go:171","msg":"trace[1309557755] transaction","detail":"{read_only:false; number_of_response:1; response_revision:1206; }","duration":"202.831779ms","start":"2024-01-20T00:13:35.319161Z","end":"2024-01-20T00:13:35.521993Z","steps":["trace[1309557755] 'process raft request'  (duration: 199.749111ms)"],"step_count":1}
{"level":"info","ts":"2024-01-20T00:13:35.723388Z","caller":"traceutil/trace.go:171","msg":"trace[1617395812] transaction","detail":"{read_only:false; response_revision:1207; number_of_response:1; }","duration":"115.119278ms","start":"2024-01-20T00:13:35.607918Z","end":"2024-01-20T00:13:35.723037Z","steps":["trace[1617395812] 'process raft request'  (duration: 99.526468ms)","trace[1617395812] 'compare'  (duration: 12.628226ms)"],"step_count":2}
{"level":"warn","ts":"2024-01-20T00:13:37.939581Z","caller":"etcdserver/v3_server.go:897","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":3238524793671030412,"retry-timeout":"500ms"}
{"level":"warn","ts":"2024-01-20T00:13:38.440138Z","caller":"etcdserver/v3_server.go:897","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":3238524793671030412,"retry-timeout":"500ms"}
{"level":"info","ts":"2024-01-20T00:13:38.813003Z","caller":"traceutil/trace.go:171","msg":"trace[301709881] linearizableReadLoop","detail":"{readStateIndex:1387; appliedIndex:1386; }","duration":"1.393744261s","start":"2024-01-20T00:13:37.41921Z","end":"2024-01-20T00:13:38.812955Z","steps":["trace[301709881] 'read index received'  (duration: 1.390668741s)","trace[301709881] 'applied index is now lower than readState.Index'  (duration: 3.074092ms)"],"step_count":2}
{"level":"info","ts":"2024-01-20T00:13:38.81329Z","caller":"traceutil/trace.go:171","msg":"trace[1934611199] transaction","detail":"{read_only:false; response_revision:1209; number_of_response:1; }","duration":"1.494551313s","start":"2024-01-20T00:13:37.31867Z","end":"2024-01-20T00:13:38.813221Z","steps":["trace[1934611199] 'process raft request'  (duration: 1.493501073s)"],"step_count":1}
{"level":"warn","ts":"2024-01-20T00:13:38.813597Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-20T00:13:37.318653Z","time spent":"1.494728035s","remote":"127.0.0.1:51906","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":5060,"response count":0,"response size":39,"request content":"compare:<target:MOD key:\"/registry/minions/minikube\" mod_revision:1164 > success:<request_put:<key:\"/registry/minions/minikube\" value_size:5026 >> failure:<request_range:<key:\"/registry/minions/minikube\" > >"}
{"level":"warn","ts":"2024-01-20T00:13:38.813988Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.394775687s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-01-20T00:13:38.814126Z","caller":"traceutil/trace.go:171","msg":"trace[1615512733] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:1209; }","duration":"1.394894525s","start":"2024-01-20T00:13:37.419182Z","end":"2024-01-20T00:13:38.814077Z","steps":["trace[1615512733] 'agreement among raft nodes before linearized reading'  (duration: 1.394672621s)"],"step_count":1}
{"level":"warn","ts":"2024-01-20T00:13:38.814197Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-20T00:13:37.419165Z","time spent":"1.395018738s","remote":"127.0.0.1:51764","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":28,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2024-01-20T00:13:38.814859Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.125346251s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/flowschemas/\" range_end:\"/registry/flowschemas0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2024-01-20T00:13:38.814912Z","caller":"traceutil/trace.go:171","msg":"trace[7238423] range","detail":"{range_begin:/registry/flowschemas/; range_end:/registry/flowschemas0; response_count:0; response_revision:1209; }","duration":"1.125435928s","start":"2024-01-20T00:13:37.689461Z","end":"2024-01-20T00:13:38.814897Z","steps":["trace[7238423] 'agreement among raft nodes before linearized reading'  (duration: 1.125293094s)"],"step_count":1}
{"level":"warn","ts":"2024-01-20T00:13:38.814955Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-20T00:13:37.689445Z","time spent":"1.125499019s","remote":"127.0.0.1:52140","response type":"/etcdserverpb.KV/Range","request count":0,"request size":50,"response count":13,"response size":30,"request content":"key:\"/registry/flowschemas/\" range_end:\"/registry/flowschemas0\" count_only:true "}
{"level":"warn","ts":"2024-01-20T00:13:39.606191Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"116.676691ms","expected-duration":"100ms","prefix":"","request":"header:<ID:3238524793671030425 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/default/mongo-express-service\" mod_revision:0 > success:<request_put:<key:\"/registry/services/endpoints/default/mongo-express-service\" value_size:357 >> failure:<>>","response":"size:16"}
{"level":"info","ts":"2024-01-20T00:13:39.606447Z","caller":"traceutil/trace.go:171","msg":"trace[465762768] linearizableReadLoop","detail":"{readStateIndex:1392; appliedIndex:1390; }","duration":"185.567606ms","start":"2024-01-20T00:13:39.420864Z","end":"2024-01-20T00:13:39.606432Z","steps":["trace[465762768] 'read index received'  (duration: 5.241603ms)","trace[465762768] 'applied index is now lower than readState.Index'  (duration: 180.324846ms)"],"step_count":2}
{"level":"info","ts":"2024-01-20T00:13:39.606704Z","caller":"traceutil/trace.go:171","msg":"trace[1723985196] transaction","detail":"{read_only:false; response_revision:1214; number_of_response:1; }","duration":"267.96471ms","start":"2024-01-20T00:13:39.33872Z","end":"2024-01-20T00:13:39.606685Z","steps":["trace[1723985196] 'process raft request'  (duration: 267.611439ms)"],"step_count":1}
{"level":"info","ts":"2024-01-20T00:13:39.606737Z","caller":"traceutil/trace.go:171","msg":"trace[1629145329] transaction","detail":"{read_only:false; response_revision:1213; number_of_response:1; }","duration":"272.790404ms","start":"2024-01-20T00:13:39.333935Z","end":"2024-01-20T00:13:39.606725Z","steps":["trace[1629145329] 'process raft request'  (duration: 92.232513ms)","trace[1629145329] 'compare'  (duration: 113.819098ms)"],"step_count":2}
{"level":"warn","ts":"2024-01-20T00:13:39.608274Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"187.418427ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/kube-system/storage-provisioner\" ","response":"range_response_count:1 size:3922"}
{"level":"info","ts":"2024-01-20T00:13:39.608388Z","caller":"traceutil/trace.go:171","msg":"trace[2115485071] range","detail":"{range_begin:/registry/pods/kube-system/storage-provisioner; range_end:; response_count:1; response_revision:1214; }","duration":"187.538159ms","start":"2024-01-20T00:13:39.420832Z","end":"2024-01-20T00:13:39.60837Z","steps":["trace[2115485071] 'agreement among raft nodes before linearized reading'  (duration: 185.655673ms)"],"step_count":1}
{"level":"info","ts":"2024-01-20T00:13:52.874357Z","caller":"traceutil/trace.go:171","msg":"trace[2066606798] transaction","detail":"{read_only:false; response_revision:1221; number_of_response:1; }","duration":"122.920834ms","start":"2024-01-20T00:13:52.751415Z","end":"2024-01-20T00:13:52.874336Z","steps":["trace[2066606798] 'process raft request'  (duration: 122.577878ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-20T00:14:00.313081Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"106.462242ms","expected-duration":"100ms","prefix":"","request":"header:<ID:3238524793671030511 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.58.2\" mod_revision:1220 > success:<request_put:<key:\"/registry/masterleases/192.168.58.2\" value_size:65 lease:3238524793671030509 >> failure:<request_range:<key:\"/registry/masterleases/192.168.58.2\" > >>","response":"size:16"}
{"level":"info","ts":"2024-01-20T00:14:00.31485Z","caller":"traceutil/trace.go:171","msg":"trace[1220713591] transaction","detail":"{read_only:false; response_revision:1225; number_of_response:1; }","duration":"388.710575ms","start":"2024-01-20T00:13:59.926112Z","end":"2024-01-20T00:14:00.314823Z","steps":["trace[1220713591] 'process raft request'  (duration: 280.238714ms)","trace[1220713591] 'compare'  (duration: 106.293043ms)"],"step_count":2}
{"level":"warn","ts":"2024-01-20T00:14:00.315059Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-20T00:13:59.926028Z","time spent":"388.936659ms","remote":"127.0.0.1:51792","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":116,"response count":0,"response size":39,"request content":"compare:<target:MOD key:\"/registry/masterleases/192.168.58.2\" mod_revision:1220 > success:<request_put:<key:\"/registry/masterleases/192.168.58.2\" value_size:65 lease:3238524793671030509 >> failure:<request_range:<key:\"/registry/masterleases/192.168.58.2\" > >"}
{"level":"warn","ts":"2024-01-20T00:14:05.160103Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"150.870551ms","expected-duration":"100ms","prefix":"","request":"header:<ID:3238524793671030529 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<key:\"compact_rev_key\" version:0 > success:<request_put:<key:\"compact_rev_key\" value_size:1 >> failure:<request_range:<key:\"compact_rev_key\" > >>","response":"size:48"}
{"level":"info","ts":"2024-01-20T00:14:05.160603Z","caller":"traceutil/trace.go:171","msg":"trace[2000163148] transaction","detail":"{read_only:false; number_of_response:1; response_revision:1226; }","duration":"546.902876ms","start":"2024-01-20T00:14:04.613673Z","end":"2024-01-20T00:14:05.160576Z","steps":["trace[2000163148] 'process raft request'  (duration: 395.45824ms)","trace[2000163148] 'compare'  (duration: 150.762034ms)"],"step_count":2}
{"level":"warn","ts":"2024-01-20T00:14:05.160753Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-20T00:14:04.613629Z","time spent":"547.026159ms","remote":"127.0.0.1:51782","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":19,"response count":0,"response size":71,"request content":"compare:<key:\"compact_rev_key\" version:0 > success:<request_put:<key:\"compact_rev_key\" value_size:1 >> failure:<request_range:<key:\"compact_rev_key\" > >"}
{"level":"info","ts":"2024-01-20T00:14:05.163171Z","caller":"traceutil/trace.go:171","msg":"trace[49609471] linearizableReadLoop","detail":"{readStateIndex:1412; appliedIndex:1411; }","duration":"133.499521ms","start":"2024-01-20T00:14:05.029644Z","end":"2024-01-20T00:14:05.163144Z","steps":["trace[49609471] 'read index received'  (duration: 133.182263ms)","trace[49609471] 'applied index is now lower than readState.Index'  (duration: 315.509¬µs)"],"step_count":2}
{"level":"warn","ts":"2024-01-20T00:14:05.163371Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"133.711641ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/ingress/\" range_end:\"/registry/ingress0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-01-20T00:14:05.163558Z","caller":"traceutil/trace.go:171","msg":"trace[1195085963] range","detail":"{range_begin:/registry/ingress/; range_end:/registry/ingress0; response_count:0; response_revision:1226; }","duration":"133.941524ms","start":"2024-01-20T00:14:05.029602Z","end":"2024-01-20T00:14:05.163544Z","steps":["trace[1195085963] 'agreement among raft nodes before linearized reading'  (duration: 133.669894ms)"],"step_count":1}
{"level":"info","ts":"2024-01-20T00:14:26.940465Z","caller":"traceutil/trace.go:171","msg":"trace[39250066] transaction","detail":"{read_only:false; response_revision:1240; number_of_response:1; }","duration":"241.944633ms","start":"2024-01-20T00:14:26.698475Z","end":"2024-01-20T00:14:26.940419Z","steps":["trace[39250066] 'process raft request'  (duration: 241.71274ms)"],"step_count":1}
{"level":"info","ts":"2024-01-20T00:14:47.290415Z","caller":"traceutil/trace.go:171","msg":"trace[1549657825] transaction","detail":"{read_only:false; response_revision:1256; number_of_response:1; }","duration":"414.979256ms","start":"2024-01-20T00:14:46.875371Z","end":"2024-01-20T00:14:47.290351Z","steps":["trace[1549657825] 'process raft request'  (duration: 414.757864ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-20T00:14:47.290769Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-20T00:14:46.875345Z","time spent":"415.239771ms","remote":"127.0.0.1:52000","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":673,"response count":0,"response size":39,"request content":"compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:1246 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:600 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >"}
{"level":"warn","ts":"2024-01-20T00:14:48.363876Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-20T00:14:47.929929Z","time spent":"433.93762ms","remote":"127.0.0.1:36586","response type":"/etcdserverpb.Maintenance/Status","request count":-1,"request size":-1,"response count":-1,"response size":-1,"request content":""}
{"level":"warn","ts":"2024-01-20T00:14:48.36404Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"423.119953ms","expected-duration":"100ms","prefix":"","request":"header:<ID:3238524793671030689 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:1248 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >>","response":"size:16"}
{"level":"info","ts":"2024-01-20T00:14:48.364151Z","caller":"traceutil/trace.go:171","msg":"trace[9488297] linearizableReadLoop","detail":"{readStateIndex:1452; appliedIndex:1451; }","duration":"138.529923ms","start":"2024-01-20T00:14:48.225593Z","end":"2024-01-20T00:14:48.364123Z","steps":["trace[9488297] 'read index received'  (duration: 36.957¬µs)","trace[9488297] 'applied index is now lower than readState.Index'  (duration: 138.491359ms)"],"step_count":2}
{"level":"warn","ts":"2024-01-20T00:14:48.364334Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"138.757963ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/secrets/\" range_end:\"/registry/secrets0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2024-01-20T00:14:48.364467Z","caller":"traceutil/trace.go:171","msg":"trace[364099458] transaction","detail":"{read_only:false; response_revision:1257; number_of_response:1; }","duration":"932.952302ms","start":"2024-01-20T00:14:47.431497Z","end":"2024-01-20T00:14:48.364449Z","steps":["trace[364099458] 'process raft request'  (duration: 509.329494ms)","trace[364099458] 'compare'  (duration: 422.913928ms)"],"step_count":2}
{"level":"warn","ts":"2024-01-20T00:14:48.364575Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-20T00:14:47.43148Z","time spent":"933.05713ms","remote":"127.0.0.1:52000","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":521,"response count":0,"response size":39,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:1248 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}
{"level":"info","ts":"2024-01-20T00:14:48.367374Z","caller":"traceutil/trace.go:171","msg":"trace[2125319962] range","detail":"{range_begin:/registry/secrets/; range_end:/registry/secrets0; response_count:0; response_revision:1257; }","duration":"138.799004ms","start":"2024-01-20T00:14:48.225558Z","end":"2024-01-20T00:14:48.364357Z","steps":["trace[2125319962] 'agreement among raft nodes before linearized reading'  (duration: 138.630689ms)"],"step_count":1}

* 
* ==> etcd [bf65849b8305] <==
* {"level":"info","ts":"2024-01-19T04:20:59.273099Z","caller":"traceutil/trace.go:171","msg":"trace[725087552] range","detail":"{range_begin:/registry/namespaces/; range_end:/registry/namespaces0; response_count:0; response_revision:738; }","duration":"193.242549ms","start":"2024-01-19T04:20:59.079851Z","end":"2024-01-19T04:20:59.273093Z","steps":["trace[725087552] 'agreement among raft nodes before linearized reading'  (duration: 193.205144ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-19T04:20:59.273157Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"189.730486ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/jobs/\" range_end:\"/registry/jobs0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-01-19T04:20:59.273178Z","caller":"traceutil/trace.go:171","msg":"trace[1681218442] range","detail":"{range_begin:/registry/jobs/; range_end:/registry/jobs0; response_count:0; response_revision:738; }","duration":"189.754945ms","start":"2024-01-19T04:20:59.083417Z","end":"2024-01-19T04:20:59.273172Z","steps":["trace[1681218442] 'agreement among raft nodes before linearized reading'  (duration: 189.717778ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-19T04:20:59.273204Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"193.389198ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/networkpolicies/\" range_end:\"/registry/networkpolicies0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-01-19T04:20:59.273226Z","caller":"traceutil/trace.go:171","msg":"trace[1618301407] range","detail":"{range_begin:/registry/networkpolicies/; range_end:/registry/networkpolicies0; response_count:0; response_revision:738; }","duration":"193.412919ms","start":"2024-01-19T04:20:59.079807Z","end":"2024-01-19T04:20:59.27322Z","steps":["trace[1618301407] 'agreement among raft nodes before linearized reading'  (duration: 193.376193ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-19T04:20:59.273287Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"189.893344ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/ingressclasses/\" range_end:\"/registry/ingressclasses0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-01-19T04:20:59.273307Z","caller":"traceutil/trace.go:171","msg":"trace[2137612558] range","detail":"{range_begin:/registry/ingressclasses/; range_end:/registry/ingressclasses0; response_count:0; response_revision:738; }","duration":"189.91629ms","start":"2024-01-19T04:20:59.083386Z","end":"2024-01-19T04:20:59.273302Z","steps":["trace[2137612558] 'agreement among raft nodes before linearized reading'  (duration: 189.878982ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-19T04:20:59.273328Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"195.063843ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/persistentvolumes/\" range_end:\"/registry/persistentvolumes0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-01-19T04:20:59.273347Z","caller":"traceutil/trace.go:171","msg":"trace[70390115] range","detail":"{range_begin:/registry/persistentvolumes/; range_end:/registry/persistentvolumes0; response_count:0; response_revision:738; }","duration":"195.092259ms","start":"2024-01-19T04:20:59.07825Z","end":"2024-01-19T04:20:59.273342Z","steps":["trace[70390115] 'agreement among raft nodes before linearized reading'  (duration: 195.058435ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-19T04:20:59.273413Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"190.047778ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/volumeattachments/\" range_end:\"/registry/volumeattachments0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-01-19T04:20:59.273432Z","caller":"traceutil/trace.go:171","msg":"trace[331789613] range","detail":"{range_begin:/registry/volumeattachments/; range_end:/registry/volumeattachments0; response_count:0; response_revision:738; }","duration":"190.073124ms","start":"2024-01-19T04:20:59.083354Z","end":"2024-01-19T04:20:59.273427Z","steps":["trace[331789613] 'agreement among raft nodes before linearized reading'  (duration: 190.034812ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-19T04:20:59.273452Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"195.364369ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/horizontalpodautoscalers/\" range_end:\"/registry/horizontalpodautoscalers0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-01-19T04:20:59.273474Z","caller":"traceutil/trace.go:171","msg":"trace[402994844] range","detail":"{range_begin:/registry/horizontalpodautoscalers/; range_end:/registry/horizontalpodautoscalers0; response_count:0; response_revision:738; }","duration":"195.388396ms","start":"2024-01-19T04:20:59.07808Z","end":"2024-01-19T04:20:59.273468Z","steps":["trace[402994844] 'agreement among raft nodes before linearized reading'  (duration: 195.352614ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-19T04:20:59.273538Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"190.178084ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/leases/\" range_end:\"/registry/leases0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2024-01-19T04:20:59.273558Z","caller":"traceutil/trace.go:171","msg":"trace[91936823] range","detail":"{range_begin:/registry/leases/; range_end:/registry/leases0; response_count:0; response_revision:738; }","duration":"190.201762ms","start":"2024-01-19T04:20:59.083352Z","end":"2024-01-19T04:20:59.273553Z","steps":["trace[91936823] 'agreement among raft nodes before linearized reading'  (duration: 190.164862ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-19T04:20:59.273575Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"195.529311ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/192.168.58.2\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-01-19T04:20:59.273595Z","caller":"traceutil/trace.go:171","msg":"trace[1075738931] range","detail":"{range_begin:/registry/masterleases/192.168.58.2; range_end:; response_count:0; response_revision:738; }","duration":"195.549898ms","start":"2024-01-19T04:20:59.078039Z","end":"2024-01-19T04:20:59.273589Z","steps":["trace[1075738931] 'agreement among raft nodes before linearized reading'  (duration: 195.517856ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-19T04:20:59.273673Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"190.426512ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/apiregistration.k8s.io/apiservices/\" range_end:\"/registry/apiregistration.k8s.io/apiservices0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2024-01-19T04:20:59.273694Z","caller":"traceutil/trace.go:171","msg":"trace[595347832] range","detail":"{range_begin:/registry/apiregistration.k8s.io/apiservices/; range_end:/registry/apiregistration.k8s.io/apiservices0; response_count:0; response_revision:738; }","duration":"190.450083ms","start":"2024-01-19T04:20:59.083238Z","end":"2024-01-19T04:20:59.273688Z","steps":["trace[595347832] 'agreement among raft nodes before linearized reading'  (duration: 190.40591ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-19T04:20:59.274544Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"191.307597ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/\" range_end:\"/registry/serviceaccounts0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2024-01-19T04:20:59.280442Z","caller":"traceutil/trace.go:171","msg":"trace[796184277] range","detail":"{range_begin:/registry/serviceaccounts/; range_end:/registry/serviceaccounts0; response_count:0; response_revision:738; }","duration":"197.16624ms","start":"2024-01-19T04:20:59.083207Z","end":"2024-01-19T04:20:59.280373Z","steps":["trace[796184277] 'agreement among raft nodes before linearized reading'  (duration: 191.238042ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-19T04:20:59.274628Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"191.567452ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/roles/\" range_end:\"/registry/roles0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2024-01-19T04:20:59.280638Z","caller":"traceutil/trace.go:171","msg":"trace[2024570755] range","detail":"{range_begin:/registry/roles/; range_end:/registry/roles0; response_count:0; response_revision:738; }","duration":"197.578146ms","start":"2024-01-19T04:20:59.08305Z","end":"2024-01-19T04:20:59.280628Z","steps":["trace[2024570755] 'agreement among raft nodes before linearized reading'  (duration: 191.538775ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-19T04:20:59.274686Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"191.965621ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/ingress/\" range_end:\"/registry/ingress0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-01-19T04:20:59.280815Z","caller":"traceutil/trace.go:171","msg":"trace[256242588] range","detail":"{range_begin:/registry/ingress/; range_end:/registry/ingress0; response_count:0; response_revision:738; }","duration":"198.094986ms","start":"2024-01-19T04:20:59.082712Z","end":"2024-01-19T04:20:59.280807Z","steps":["trace[256242588] 'agreement among raft nodes before linearized reading'  (duration: 191.942736ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-19T04:20:59.274726Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"192.04575ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/priorityclasses/\" range_end:\"/registry/priorityclasses0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2024-01-19T04:20:59.281003Z","caller":"traceutil/trace.go:171","msg":"trace[1415227766] range","detail":"{range_begin:/registry/priorityclasses/; range_end:/registry/priorityclasses0; response_count:0; response_revision:738; }","duration":"198.32166ms","start":"2024-01-19T04:20:59.082674Z","end":"2024-01-19T04:20:59.280995Z","steps":["trace[1415227766] 'agreement among raft nodes before linearized reading'  (duration: 192.02577ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-19T04:20:59.274856Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"192.170788ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/\" range_end:\"/registry/pods0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2024-01-19T04:20:59.281202Z","caller":"traceutil/trace.go:171","msg":"trace[1857149282] range","detail":"{range_begin:/registry/pods/; range_end:/registry/pods0; response_count:0; response_revision:738; }","duration":"198.524489ms","start":"2024-01-19T04:20:59.08267Z","end":"2024-01-19T04:20:59.281194Z","steps":["trace[1857149282] 'agreement among raft nodes before linearized reading'  (duration: 192.077278ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-19T04:20:59.274915Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"192.242168ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/poddisruptionbudgets/\" range_end:\"/registry/poddisruptionbudgets0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-01-19T04:20:59.281409Z","caller":"traceutil/trace.go:171","msg":"trace[906726112] range","detail":"{range_begin:/registry/poddisruptionbudgets/; range_end:/registry/poddisruptionbudgets0; response_count:0; response_revision:738; }","duration":"198.751034ms","start":"2024-01-19T04:20:59.082645Z","end":"2024-01-19T04:20:59.281396Z","steps":["trace[906726112] 'agreement among raft nodes before linearized reading'  (duration: 192.224936ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-19T04:20:59.277303Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"199.311195ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/runtimeclasses/\" range_end:\"/registry/runtimeclasses0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-01-19T04:20:59.281784Z","caller":"traceutil/trace.go:171","msg":"trace[383908758] range","detail":"{range_begin:/registry/runtimeclasses/; range_end:/registry/runtimeclasses0; response_count:0; response_revision:738; }","duration":"203.805669ms","start":"2024-01-19T04:20:59.077968Z","end":"2024-01-19T04:20:59.281773Z","steps":["trace[383908758] 'agreement among raft nodes before linearized reading'  (duration: 199.280493ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-19T04:20:59.277365Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"192.981134ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/statefulsets/\" range_end:\"/registry/statefulsets0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-01-19T04:20:59.281918Z","caller":"traceutil/trace.go:171","msg":"trace[1107343219] range","detail":"{range_begin:/registry/statefulsets/; range_end:/registry/statefulsets0; response_count:0; response_revision:738; }","duration":"197.53629ms","start":"2024-01-19T04:20:59.084374Z","end":"2024-01-19T04:20:59.281911Z","steps":["trace[1107343219] 'agreement among raft nodes before linearized reading'  (duration: 192.970392ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-19T04:20:59.277731Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"192.937766ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/prioritylevelconfigurations/\" range_end:\"/registry/prioritylevelconfigurations0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2024-01-19T04:20:59.282084Z","caller":"traceutil/trace.go:171","msg":"trace[417568503] range","detail":"{range_begin:/registry/prioritylevelconfigurations/; range_end:/registry/prioritylevelconfigurations0; response_count:0; response_revision:738; }","duration":"197.29223ms","start":"2024-01-19T04:20:59.084783Z","end":"2024-01-19T04:20:59.282075Z","steps":["trace[417568503] 'agreement among raft nodes before linearized reading'  (duration: 192.917879ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-19T04:20:59.2737Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"195.691839ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/secrets/\" range_end:\"/registry/secrets0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2024-01-19T04:20:59.282177Z","caller":"traceutil/trace.go:171","msg":"trace[1062622481] range","detail":"{range_begin:/registry/secrets/; range_end:/registry/secrets0; response_count:0; response_revision:738; }","duration":"204.169979ms","start":"2024-01-19T04:20:59.078Z","end":"2024-01-19T04:20:59.28217Z","steps":["trace[1062622481] 'agreement among raft nodes before linearized reading'  (duration: 195.678428ms)"],"step_count":1}
{"level":"info","ts":"2024-01-19T04:20:59.380361Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":589}
{"level":"info","ts":"2024-01-19T04:20:59.381493Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":589,"took":"552.227¬µs","hash":1768590906}
{"level":"info","ts":"2024-01-19T04:20:59.381538Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1768590906,"revision":589,"compact-revision":456}
{"level":"info","ts":"2024-01-19T04:20:59.496682Z","caller":"traceutil/trace.go:171","msg":"trace[885734852] transaction","detail":"{read_only:false; response_revision:741; number_of_response:1; }","duration":"100.057126ms","start":"2024-01-19T04:20:59.396609Z","end":"2024-01-19T04:20:59.496667Z","steps":["trace[885734852] 'process raft request'  (duration: 100.019063ms)"],"step_count":1}
{"level":"info","ts":"2024-01-19T04:20:59.4969Z","caller":"traceutil/trace.go:171","msg":"trace[419896301] transaction","detail":"{read_only:false; response_revision:740; number_of_response:1; }","duration":"107.424705ms","start":"2024-01-19T04:20:59.389465Z","end":"2024-01-19T04:20:59.496889Z","steps":["trace[419896301] 'process raft request'  (duration: 103.069064ms)"],"step_count":1}
{"level":"info","ts":"2024-01-19T04:20:59.49705Z","caller":"traceutil/trace.go:171","msg":"trace[1282716672] linearizableReadLoop","detail":"{readStateIndex:831; appliedIndex:830; }","duration":"102.446678ms","start":"2024-01-19T04:20:59.394594Z","end":"2024-01-19T04:20:59.49704Z","steps":["trace[1282716672] 'read index received'  (duration: 97.887041ms)","trace[1282716672] 'applied index is now lower than readState.Index'  (duration: 4.558318ms)"],"step_count":2}
{"level":"warn","ts":"2024-01-19T04:20:59.569161Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"172.909813ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-01-19T04:20:59.569218Z","caller":"traceutil/trace.go:171","msg":"trace[869997860] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:741; }","duration":"172.979383ms","start":"2024-01-19T04:20:59.396225Z","end":"2024-01-19T04:20:59.569204Z","steps":["trace[869997860] 'agreement among raft nodes before linearized reading'  (duration: 172.799689ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-19T04:20:59.569245Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"174.875069ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/certificate-controller\" ","response":"range_response_count:1 size:209"}
{"level":"info","ts":"2024-01-19T04:20:59.569282Z","caller":"traceutil/trace.go:171","msg":"trace[204581563] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/certificate-controller; range_end:; response_count:1; response_revision:741; }","duration":"174.910711ms","start":"2024-01-19T04:20:59.394358Z","end":"2024-01-19T04:20:59.569269Z","steps":["trace[204581563] 'agreement among raft nodes before linearized reading'  (duration: 174.841772ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-19T04:20:59.569424Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"172.644544ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/specs/\" range_end:\"/registry/services/specs0\" ","response":"range_response_count:2 size:1908"}
{"level":"info","ts":"2024-01-19T04:20:59.569449Z","caller":"traceutil/trace.go:171","msg":"trace[588055972] range","detail":"{range_begin:/registry/services/specs/; range_end:/registry/services/specs0; response_count:2; response_revision:741; }","duration":"172.67446ms","start":"2024-01-19T04:20:59.396768Z","end":"2024-01-19T04:20:59.569442Z","steps":["trace[588055972] 'agreement among raft nodes before linearized reading'  (duration: 172.611948ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-19T04:20:59.569161Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"174.767943ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-01-19T04:20:59.569518Z","caller":"traceutil/trace.go:171","msg":"trace[726519409] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:741; }","duration":"175.134457ms","start":"2024-01-19T04:20:59.394376Z","end":"2024-01-19T04:20:59.56951Z","steps":["trace[726519409] 'agreement among raft nodes before linearized reading'  (duration: 174.704428ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-19T04:20:59.569859Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"180.58363ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/node-controller\" ","response":"range_response_count:1 size:195"}
{"level":"info","ts":"2024-01-19T04:20:59.569887Z","caller":"traceutil/trace.go:171","msg":"trace[1370897991] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/node-controller; range_end:; response_count:1; response_revision:741; }","duration":"180.617622ms","start":"2024-01-19T04:20:59.389262Z","end":"2024-01-19T04:20:59.56988Z","steps":["trace[1370897991] 'agreement among raft nodes before linearized reading'  (duration: 180.553711ms)"],"step_count":1}
{"level":"info","ts":"2024-01-19T04:21:10.08168Z","caller":"traceutil/trace.go:171","msg":"trace[1814271223] transaction","detail":"{read_only:false; response_revision:755; number_of_response:1; }","duration":"191.717222ms","start":"2024-01-19T04:21:09.889593Z","end":"2024-01-19T04:21:10.08131Z","steps":["trace[1814271223] 'process raft request'  (duration: 116.943146ms)","trace[1814271223] 'store kv pair into bolt db' {req_type:put; key:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; req_size:582; } (duration: 74.022698ms)"],"step_count":2}
{"level":"info","ts":"2024-01-19T04:21:10.202977Z","caller":"traceutil/trace.go:171","msg":"trace[1008789122] transaction","detail":"{read_only:false; response_revision:757; number_of_response:1; }","duration":"223.774933ms","start":"2024-01-19T04:21:09.97911Z","end":"2024-01-19T04:21:10.202885Z","steps":["trace[1008789122] 'process raft request'  (duration: 221.464597ms)"],"step_count":1}
{"level":"info","ts":"2024-01-19T04:21:10.204141Z","caller":"traceutil/trace.go:171","msg":"trace[1220383476] transaction","detail":"{read_only:false; response_revision:756; number_of_response:1; }","duration":"305.2679ms","start":"2024-01-19T04:21:09.898412Z","end":"2024-01-19T04:21:10.203679Z","steps":["trace[1220383476] 'process raft request'  (duration: 279.399771ms)","trace[1220383476] 'compare'  (duration: 21.903786ms)"],"step_count":2}
{"level":"warn","ts":"2024-01-19T04:21:10.205023Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-19T04:21:09.89832Z","time spent":"306.011897ms","remote":"127.0.0.1:41802","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":673,"response count":0,"response size":39,"request content":"compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:742 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:600 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >"}
{"level":"info","ts":"2024-01-19T04:21:12.292754Z","caller":"traceutil/trace.go:171","msg":"trace[1354526985] transaction","detail":"{read_only:false; response_revision:760; number_of_response:1; }","duration":"100.169106ms","start":"2024-01-19T04:21:12.192503Z","end":"2024-01-19T04:21:12.292672Z","steps":["trace[1354526985] 'process raft request'  (duration: 98.005672ms)"],"step_count":1}

* 
* ==> kernel <==
*  00:17:47 up 9 min,  0 users,  load average: 1.28, 1.35, 0.80
Linux minikube 6.4.16-linuxkit #1 SMP PREEMPT_DYNAMIC Sat Sep 23 13:39:52 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.3 LTS"

* 
* ==> kube-apiserver [1796c8064f63] <==
* Trace[139361749]: ---"Transaction prepared" 1126ms (00:12:20.856)
Trace[139361749]: [1.129817963s] [1.129817963s] END
I0120 00:12:35.579434       1 trace.go:236] Trace[1921461875]: "Get" accept:application/json, */*,audit-id:b1dc1b12-cf72-4902-ae6c-f4a68187ae6c,client:192.168.58.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:GET (20-Jan-2024 00:12:33.483) (total time: 2093ms):
Trace[1921461875]: ---"About to write a response" 2093ms (00:12:35.577)
Trace[1921461875]: [2.093403253s] [2.093403253s] END
I0120 00:12:53.209429       1 trace.go:236] Trace[1296540868]: "Update" accept:application/json, */*,audit-id:e222ba32-1d1b-47af-a8c5-757a3780a17d,client:192.168.58.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (20-Jan-2024 00:12:52.654) (total time: 555ms):
Trace[1296540868]: ["GuaranteedUpdate etcd3" audit-id:e222ba32-1d1b-47af-a8c5-757a3780a17d,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 554ms (00:12:52.654)
Trace[1296540868]:  ---"Txn call completed" 551ms (00:12:53.209)]
Trace[1296540868]: [555.064689ms] [555.064689ms] END
I0120 00:13:00.409192       1 trace.go:236] Trace[578011232]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:790aeb1c-cbee-46b2-bb21-f1b60d0ace81,client:192.168.58.2,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:PUT (20-Jan-2024 00:12:59.011) (total time: 1316ms):
Trace[578011232]: ["GuaranteedUpdate etcd3" audit-id:790aeb1c-cbee-46b2-bb21-f1b60d0ace81,key:/leases/kube-node-lease/minikube,type:*coordination.Lease,resource:leases.coordination.k8s.io 1309ms (00:12:59.018)
Trace[578011232]:  ---"About to Encode" 394ms (00:12:59.416)
Trace[578011232]:  ---"Txn call completed" 909ms (00:13:00.326)]
Trace[578011232]: [1.316450644s] [1.316450644s] END
E0120 00:13:34.620551       1 status.go:71] apiserver received an error that is not an metav1.Status: rpctypes.EtcdError{code:0xe, desc:"etcdserver: request timed out"}: etcdserver: request timed out
I0120 00:13:34.621368       1 trace.go:236] Trace[2134676064]: "Update" accept:application/json, */*,audit-id:1581d78e-a5d9-43f9-a93f-6560e3a721ea,client:192.168.58.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (20-Jan-2024 00:13:26.006) (total time: 8877ms):
Trace[2134676064]: ["GuaranteedUpdate etcd3" audit-id:1581d78e-a5d9-43f9-a93f-6560e3a721ea,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 8877ms (00:13:26.006)
Trace[2134676064]:  ---"Txn call failed" err:etcdserver: request timed out 8611ms (00:13:34.420)]
Trace[2134676064]: [8.877733292s] [8.877733292s] END
I0120 00:13:34.637184       1 trace.go:236] Trace[1410491380]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:6d4a9274-d7d1-4a6a-ab05-6f3aa00f749f,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,user-agent:kube-apiserver/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:PUT (20-Jan-2024 00:13:29.641) (total time: 5258ms):
Trace[1410491380]: ["GuaranteedUpdate etcd3" audit-id:6d4a9274-d7d1-4a6a-ab05-6f3aa00f749f,key:/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,type:*coordination.Lease,resource:leases.coordination.k8s.io 5254ms (00:13:29.645)
Trace[1410491380]:  ---"Txn call completed" 5243ms (00:13:34.630)]
Trace[1410491380]: [5.258161431s] [5.258161431s] END
I0120 00:13:35.507752       1 trace.go:236] Trace[1727563358]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:9ab30e1c-2385-44f7-9f1b-85eb623f9e00,client:192.168.58.2,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:PUT (20-Jan-2024 00:13:34.712) (total time: 795ms):
Trace[1727563358]: ["GuaranteedUpdate etcd3" audit-id:9ab30e1c-2385-44f7-9f1b-85eb623f9e00,key:/leases/kube-node-lease/minikube,type:*coordination.Lease,resource:leases.coordination.k8s.io 794ms (00:13:34.712)
Trace[1727563358]:  ---"About to Encode" 296ms (00:13:35.009)
Trace[1727563358]:  ---"Txn call completed" 497ms (00:13:35.506)]
Trace[1727563358]: [795.079442ms] [795.079442ms] END
I0120 00:13:35.730071       1 trace.go:236] Trace[1387908854]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:e5432fcc-57af-4616-bdd8-e3b8bb88301e,client:192.168.58.2,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/kube-system/events/kube-apiserver-minikube.17abe5b200099b09,user-agent:kubelet/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:PATCH (20-Jan-2024 00:13:34.517) (total time: 1212ms):
Trace[1387908854]: ["GuaranteedUpdate etcd3" audit-id:e5432fcc-57af-4616-bdd8-e3b8bb88301e,key:/events/kube-system/kube-apiserver-minikube.17abe5b200099b09,type:*core.Event,resource:events 1202ms (00:13:34.527)
Trace[1387908854]:  ---"initial value restored" 202ms (00:13:34.730)
Trace[1387908854]:  ---"About to Encode" 282ms (00:13:35.013)
Trace[1387908854]:  ---"Transaction prepared" 509ms (00:13:35.523)
Trace[1387908854]:  ---"Txn call completed" 201ms (00:13:35.725)]
Trace[1387908854]: ---"About to check admission control" 281ms (00:13:35.011)
Trace[1387908854]: ---"Object stored in database" 713ms (00:13:35.725)
Trace[1387908854]: [1.212320909s] [1.212320909s] END
I0120 00:13:35.814512       1 trace.go:236] Trace[1154783480]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.58.2,type:*v1.Endpoints,resource:apiServerIPInfo (20-Jan-2024 00:13:29.998) (total time: 6078ms):
Trace[1154783480]: ---"initial value restored" 4895ms (00:13:34.631)
Trace[1154783480]: ---"Transaction prepared" 676ms (00:13:35.307)
Trace[1154783480]: ---"Txn call completed" 300ms (00:13:35.608)
Trace[1154783480]: ---"Transaction prepared" 119ms (00:13:35.727)
Trace[1154783480]: ---"Txn call completed" 86ms (00:13:35.814)
Trace[1154783480]: [6.078523619s] [6.078523619s] END
I0120 00:13:38.819762       1 trace.go:236] Trace[461784626]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:d4a29626-bb27-400a-bf9f-aebf42137780,client:192.168.58.2,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/minikube/status,user-agent:kubelet/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:PATCH (20-Jan-2024 00:13:37.239) (total time: 1579ms):
Trace[461784626]: ["GuaranteedUpdate etcd3" audit-id:d4a29626-bb27-400a-bf9f-aebf42137780,key:/minions/minikube,type:*core.Node,resource:nodes 1578ms (00:13:37.240)
Trace[461784626]:  ---"About to Encode" 76ms (00:13:37.317)
Trace[461784626]:  ---"Txn call completed" 1496ms (00:13:38.814)]
Trace[461784626]: ---"About to check admission control" 75ms (00:13:37.316)
Trace[461784626]: ---"Object stored in database" 1499ms (00:13:38.816)
Trace[461784626]: [1.579161947s] [1.579161947s] END
I0120 00:13:39.321794       1 alloc.go:330] "allocated clusterIPs" service="default/mongo-express-service" clusterIPs={"IPv4":"10.98.11.251"}
I0120 00:14:00.316389       1 trace.go:236] Trace[430848057]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.58.2,type:*v1.Endpoints,resource:apiServerIPInfo (20-Jan-2024 00:13:59.738) (total time: 577ms):
Trace[430848057]: ---"Transaction prepared" 167ms (00:13:59.909)
Trace[430848057]: ---"Txn call completed" 406ms (00:14:00.316)
Trace[430848057]: [577.590512ms] [577.590512ms] END
I0120 00:14:48.365362       1 trace.go:236] Trace[1128855502]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:6baf48a2-08cd-4913-8086-ddb90df0ad5e,client:192.168.58.2,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:PUT (20-Jan-2024 00:14:47.424) (total time: 940ms):
Trace[1128855502]: ["GuaranteedUpdate etcd3" audit-id:6baf48a2-08cd-4913-8086-ddb90df0ad5e,key:/leases/kube-node-lease/minikube,type:*coordination.Lease,resource:leases.coordination.k8s.io 940ms (00:14:47.425)
Trace[1128855502]:  ---"Txn call completed" 934ms (00:14:48.365)]
Trace[1128855502]: [940.463273ms] [940.463273ms] END

* 
* ==> kube-apiserver [b1f1cf24b4c7] <==
* Trace[1918928458]: [7.70245063s] [7.70245063s] END
I0119 03:22:57.000476       1 trace.go:236] Trace[843690625]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:d26fb7dc-526c-416a-8119-43ce61049b64,client:192.168.58.2,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:PUT (19-Jan-2024 03:22:52.458) (total time: 4541ms):
Trace[843690625]: ["GuaranteedUpdate etcd3" audit-id:d26fb7dc-526c-416a-8119-43ce61049b64,key:/leases/kube-node-lease/minikube,type:*coordination.Lease,resource:leases.coordination.k8s.io 4541ms (03:22:52.458)
Trace[843690625]:  ---"Txn call completed" 4540ms (03:22:57.000)]
Trace[843690625]: [4.54180001s] [4.54180001s] END
I0119 03:22:57.000963       1 trace.go:236] Trace[805760918]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:09072252-e7af-4ef1-96c7-69c1703a3a81,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,user-agent:kube-apiserver/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:PUT (19-Jan-2024 03:22:52.456) (total time: 4543ms):
Trace[805760918]: ["GuaranteedUpdate etcd3" audit-id:09072252-e7af-4ef1-96c7-69c1703a3a81,key:/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,type:*coordination.Lease,resource:leases.coordination.k8s.io 4543ms (03:22:52.457)
Trace[805760918]:  ---"Txn call completed" 4542ms (03:22:57.000)]
Trace[805760918]: [4.54395672s] [4.54395672s] END
I0119 03:22:57.020973       1 trace.go:236] Trace[1608441731]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.58.2,type:*v1.Endpoints,resource:apiServerIPInfo (19-Jan-2024 03:22:52.457) (total time: 4563ms):
Trace[1608441731]: ---"initial value restored" 4544ms (03:22:57.001)
Trace[1608441731]: [4.563333415s] [4.563333415s] END
I0119 03:25:25.997287       1 trace.go:236] Trace[760183561]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:70b87e77-f226-41a5-83a0-dea65bd77bd8,client:192.168.58.2,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/minikube/status,user-agent:kubelet/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:PATCH (19-Jan-2024 03:25:25.396) (total time: 599ms):
Trace[760183561]: ["GuaranteedUpdate etcd3" audit-id:70b87e77-f226-41a5-83a0-dea65bd77bd8,key:/minions/minikube,type:*core.Node,resource:nodes 598ms (03:25:25.398)
Trace[760183561]:  ---"About to Encode" 394ms (03:25:25.793)
Trace[760183561]:  ---"Txn call completed" 199ms (03:25:25.994)]
Trace[760183561]: ---"About to check admission control" 306ms (03:25:25.705)
Trace[760183561]: ---"Object stored in database" 289ms (03:25:25.994)
Trace[760183561]: [599.986374ms] [599.986374ms] END
I0119 03:25:27.298498       1 trace.go:236] Trace[1924328310]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:09ee5f15-a2fb-47c3-8169-999decadf0a8,client:192.168.58.2,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/default/events/minikube.17aba0e43fd28e07,user-agent:kubelet/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:PATCH (19-Jan-2024 03:25:26.709) (total time: 588ms):
Trace[1924328310]: ---"limitedReadBody succeeded" len:101 85ms (03:25:26.794)
Trace[1924328310]: ["GuaranteedUpdate etcd3" audit-id:09ee5f15-a2fb-47c3-8169-999decadf0a8,key:/events/default/minikube.17aba0e43fd28e07,type:*core.Event,resource:events 503ms (03:25:26.794)
Trace[1924328310]:  ---"initial value restored" 207ms (03:25:27.002)
Trace[1924328310]:  ---"Txn call completed" 283ms (03:25:27.297)]
Trace[1924328310]: ---"Object stored in database" 285ms (03:25:27.297)
Trace[1924328310]: [588.826551ms] [588.826551ms] END
I0119 03:25:27.309629       1 trace.go:236] Trace[1618709989]: "Patch" accept:application/vnd.kubernetes.protobuf, */*,audit-id:a4c54767-8595-4050-82cf-127645e6e6ed,client:192.168.58.2,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/minikube,user-agent:kube-controller-manager/v1.28.3 (linux/amd64) kubernetes/a8a1abc/system:serviceaccount:kube-system:node-controller,verb:PATCH (19-Jan-2024 03:25:26.411) (total time: 803ms):
Trace[1618709989]: ["GuaranteedUpdate etcd3" audit-id:a4c54767-8595-4050-82cf-127645e6e6ed,key:/minions/minikube,type:*core.Node,resource:nodes 803ms (03:25:26.411)
Trace[1618709989]:  ---"About to Encode" 506ms (03:25:26.918)
Trace[1618709989]:  ---"Txn call completed" 291ms (03:25:27.210)]
Trace[1618709989]: ---"About to check admission control" 398ms (03:25:26.810)
Trace[1618709989]: ---"Object stored in database" 400ms (03:25:27.211)
Trace[1618709989]: [803.589305ms] [803.589305ms] END
I0119 03:25:43.418058       1 trace.go:236] Trace[1526982435]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.58.2,type:*v1.Endpoints,resource:apiServerIPInfo (19-Jan-2024 03:25:42.812) (total time: 604ms):
Trace[1526982435]: ---"initial value restored" 295ms (03:25:43.108)
Trace[1526982435]: ---"Txn call completed" 290ms (03:25:43.416)
Trace[1526982435]: [604.220242ms] [604.220242ms] END
I0119 03:29:25.126554       1 trace.go:236] Trace[1613697488]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:15878e01-da5c-401a-ad03-a385722b6082,client:192.168.58.2,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/minikube/status,user-agent:kubelet/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:PATCH (19-Jan-2024 03:29:24.435) (total time: 690ms):
Trace[1613697488]: ["GuaranteedUpdate etcd3" audit-id:15878e01-da5c-401a-ad03-a385722b6082,key:/minions/minikube,type:*core.Node,resource:nodes 688ms (03:29:24.437)
Trace[1613697488]:  ---"About to Encode" 383ms (03:29:24.822)
Trace[1613697488]:  ---"Txn call completed" 197ms (03:29:25.020)]
Trace[1613697488]: ---"About to check admission control" 375ms (03:29:24.813)
Trace[1613697488]: ---"Object stored in database" 208ms (03:29:25.022)
Trace[1613697488]: ---"Writing http response done" 103ms (03:29:25.126)
Trace[1613697488]: [690.538122ms] [690.538122ms] END
I0119 03:29:25.419480       1 trace.go:236] Trace[1783944904]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:35d46885-8c74-4fcb-8c17-e5d0b4032e11,client:192.168.58.2,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/default/events/minikube.17aba0e1d3da5c0d,user-agent:kubelet/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:PATCH (19-Jan-2024 03:29:24.819) (total time: 599ms):
Trace[1783944904]: ["GuaranteedUpdate etcd3" audit-id:35d46885-8c74-4fcb-8c17-e5d0b4032e11,key:/events/default/minikube.17aba0e1d3da5c0d,type:*core.Event,resource:events 597ms (03:29:24.821)
Trace[1783944904]:  ---"initial value restored" 97ms (03:29:24.919)
Trace[1783944904]:  ---"About to Encode" 220ms (03:29:25.140)
Trace[1783944904]:  ---"Txn call completed" 277ms (03:29:25.418)]
Trace[1783944904]: ---"About to check admission control" 113ms (03:29:25.032)
Trace[1783944904]: ---"Object stored in database" 385ms (03:29:25.418)
Trace[1783944904]: [599.101098ms] [599.101098ms] END
E0119 04:20:59.189235       1 timeout.go:142] post-timeout activity - time-elapsed: 2.082405992s, GET "/readyz" result: <nil>
I0119 04:20:59.285890       1 trace.go:236] Trace[1154003486]: "DeltaFIFO Pop Process" ID:v1.authentication.k8s.io,Depth:19,Reason:slow event handlers blocking the queue (19-Jan-2024 04:20:56.643) (total time: 2182ms):
Trace[1154003486]: [2.182257334s] [2.182257334s] END
I0119 04:20:59.572025       1 trace.go:236] Trace[206293086]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.58.2,type:*v1.Endpoints,resource:apiServerIPInfo (19-Jan-2024 04:20:56.648) (total time: 2463ms):
Trace[206293086]: ---"initial value restored" 2267ms (04:20:59.376)
Trace[206293086]: ---"Txn call completed" 179ms (04:20:59.571)
Trace[206293086]: [2.463299642s] [2.463299642s] END

* 
* ==> kube-controller-manager [dcfb1395d399] <==
* I0120 00:09:21.130436       1 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0120 00:09:21.135458       1 shared_informer.go:318] Caches are synced for HPA
I0120 00:09:21.135774       1 shared_informer.go:318] Caches are synced for GC
I0120 00:09:21.136777       1 shared_informer.go:318] Caches are synced for taint
I0120 00:09:21.137041       1 node_lifecycle_controller.go:1225] "Initializing eviction metric for zone" zone=""
I0120 00:09:21.137341       1 node_lifecycle_controller.go:877] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I0120 00:09:21.137474       1 node_lifecycle_controller.go:1071] "Controller detected that zone is now in new state" zone="" newState="Normal"
I0120 00:09:21.137555       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I0120 00:09:21.137637       1 taint_manager.go:211] "Sending events to api server"
I0120 00:09:21.138743       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0120 00:09:21.147981       1 shared_informer.go:318] Caches are synced for daemon sets
I0120 00:09:21.149554       1 shared_informer.go:318] Caches are synced for stateful set
I0120 00:09:21.149673       1 shared_informer.go:318] Caches are synced for attach detach
I0120 00:09:21.237910       1 shared_informer.go:318] Caches are synced for resource quota
I0120 00:09:21.310854       1 shared_informer.go:318] Caches are synced for resource quota
I0120 00:09:21.314242       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="193.562702ms"
I0120 00:09:21.314479       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="108.365¬µs"
I0120 00:09:21.318034       1 shared_informer.go:318] Caches are synced for cronjob
I0120 00:09:21.631195       1 shared_informer.go:318] Caches are synced for garbage collector
I0120 00:09:21.665276       1 shared_informer.go:318] Caches are synced for garbage collector
I0120 00:09:21.665359       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I0120 00:09:47.167091       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="11.976246ms"
I0120 00:09:47.167360       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="121.113¬µs"
I0120 00:10:11.758731       1 event.go:307] "Event occurred" object="default/mongodb-stateful-set" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="create Claim mongodb-data-mongodb-stateful-set-0 Pod mongodb-stateful-set-0 in StatefulSet mongodb-stateful-set success"
I0120 00:10:11.769245       1 event.go:307] "Event occurred" object="default/mongodb-stateful-set" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="create Pod mongodb-stateful-set-0 in StatefulSet mongodb-stateful-set successful"
I0120 00:10:11.830534       1 event.go:307] "Event occurred" object="default/mongodb-data-mongodb-stateful-set-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="Waiting for a volume to be created either by the external provisioner 'k8s.io/minikube-hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered."
I0120 00:10:11.830574       1 event.go:307] "Event occurred" object="default/mongodb-data-mongodb-stateful-set-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="Waiting for a volume to be created either by the external provisioner 'k8s.io/minikube-hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered."
I0120 00:10:20.740975       1 event.go:307] "Event occurred" object="default/note-server-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set note-server-deployment-6fb5fcb67f to 2"
I0120 00:10:20.754743       1 event.go:307] "Event occurred" object="default/note-server-deployment-6fb5fcb67f" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: note-server-deployment-6fb5fcb67f-fln28"
I0120 00:10:20.775111       1 event.go:307] "Event occurred" object="default/note-server-deployment-6fb5fcb67f" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: note-server-deployment-6fb5fcb67f-nvfsp"
I0120 00:10:20.782765       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/note-server-deployment-6fb5fcb67f" duration="42.104699ms"
I0120 00:10:20.827760       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/note-server-deployment-6fb5fcb67f" duration="44.832703ms"
I0120 00:10:20.828042       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/note-server-deployment-6fb5fcb67f" duration="104.923¬µs"
I0120 00:10:54.913826       1 event.go:307] "Event occurred" object="default/note-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set note-deployment-74cc946cd8 to 2"
I0120 00:10:54.937661       1 event.go:307] "Event occurred" object="default/note-deployment-74cc946cd8" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: note-deployment-74cc946cd8-kxxtx"
I0120 00:10:55.010787       1 event.go:307] "Event occurred" object="default/note-deployment-74cc946cd8" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: note-deployment-74cc946cd8-x6smv"
I0120 00:10:55.021791       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/note-deployment-74cc946cd8" duration="109.294442ms"
I0120 00:10:55.116113       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/note-deployment-74cc946cd8" duration="94.161154ms"
I0120 00:10:55.116358       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/note-deployment-74cc946cd8" duration="147.467¬µs"
I0120 00:10:55.128270       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/note-deployment-74cc946cd8" duration="154.517¬µs"
I0120 00:11:13.430523       1 event.go:307] "Event occurred" object="default/mongodb-stateful-set" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="create Claim mongodb-data-mongodb-stateful-set-1 Pod mongodb-stateful-set-1 in StatefulSet mongodb-stateful-set success"
I0120 00:11:13.823974       1 event.go:307] "Event occurred" object="default/mongodb-data-mongodb-stateful-set-1" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="Waiting for a volume to be created either by the external provisioner 'k8s.io/minikube-hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered."
I0120 00:11:13.824033       1 event.go:307] "Event occurred" object="default/mongodb-stateful-set" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="create Pod mongodb-stateful-set-1 in StatefulSet mongodb-stateful-set successful"
I0120 00:11:13.829038       1 event.go:307] "Event occurred" object="default/mongodb-data-mongodb-stateful-set-1" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="Waiting for a volume to be created either by the external provisioner 'k8s.io/minikube-hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered."
I0120 00:12:21.441316       1 event.go:307] "Event occurred" object="default/mongo-express-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set mongo-express-deployment-b88f6d45f to 1"
I0120 00:12:21.638647       1 event.go:307] "Event occurred" object="default/mongo-express-deployment-b88f6d45f" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: mongo-express-deployment-b88f6d45f-k2hkj"
I0120 00:12:21.808844       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/mongo-express-deployment-b88f6d45f" duration="301.438441ms"
I0120 00:12:22.015116       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/mongo-express-deployment-b88f6d45f" duration="206.163521ms"
I0120 00:12:22.015198       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/mongo-express-deployment-b88f6d45f" duration="44.01¬µs"
I0120 00:12:22.117267       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/mongo-express-deployment-b88f6d45f" duration="1.026908ms"
I0120 00:12:55.024753       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/note-server-deployment-6fb5fcb67f" duration="105.93316ms"
I0120 00:12:55.026143       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/note-server-deployment-6fb5fcb67f" duration="1.290149ms"
I0120 00:12:55.209726       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/note-server-deployment-6fb5fcb67f" duration="176.538303ms"
I0120 00:12:55.210465       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/note-server-deployment-6fb5fcb67f" duration="474.676¬µs"
I0120 00:13:11.687876       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/note-deployment-74cc946cd8" duration="96.107699ms"
I0120 00:13:11.690028       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/note-deployment-74cc946cd8" duration="1.926545ms"
I0120 00:13:11.799285       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/note-deployment-74cc946cd8" duration="25.470052ms"
I0120 00:13:11.799818       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/note-deployment-74cc946cd8" duration="270.855¬µs"
I0120 00:14:48.924131       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/mongo-express-deployment-b88f6d45f" duration="99.612503ms"
I0120 00:14:48.934990       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/mongo-express-deployment-b88f6d45f" duration="158.309¬µs"

* 
* ==> kube-controller-manager [fbcaf7940993] <==
* I0119 03:09:46.055709       1 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I0119 03:09:46.055855       1 node_lifecycle_controller.go:1225] "Initializing eviction metric for zone" zone=""
I0119 03:09:46.058071       1 node_lifecycle_controller.go:877] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I0119 03:09:46.058218       1 node_lifecycle_controller.go:1029] "Controller detected that all Nodes are not-Ready. Entering master disruption mode"
I0119 03:09:46.088875       1 shared_informer.go:318] Caches are synced for endpoint_slice
I0119 03:09:46.089114       1 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I0119 03:09:46.155959       1 shared_informer.go:318] Caches are synced for certificate-csrapproving
I0119 03:09:46.156185       1 shared_informer.go:318] Caches are synced for resource quota
I0119 03:09:46.156233       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I0119 03:09:46.156349       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I0119 03:09:46.156388       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0119 03:09:46.156530       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I0119 03:09:46.156857       1 shared_informer.go:318] Caches are synced for resource quota
I0119 03:09:46.212143       1 shared_informer.go:318] Caches are synced for attach detach
I0119 03:09:46.480611       1 event.go:307] "Event occurred" object="kube-system/kube-proxy" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: kube-proxy-7nt22"
I0119 03:09:46.584762       1 shared_informer.go:318] Caches are synced for garbage collector
I0119 03:09:46.598009       1 shared_informer.go:318] Caches are synced for garbage collector
I0119 03:09:46.598058       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I0119 03:09:46.854922       1 event.go:307] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set coredns-5dd5756b68 to 1"
I0119 03:09:47.076350       1 event.go:307] "Event occurred" object="kube-system/coredns-5dd5756b68" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: coredns-5dd5756b68-r8hd2"
I0119 03:09:47.091806       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="237.036412ms"
I0119 03:09:47.173376       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="81.494774ms"
I0119 03:09:47.173654       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="220.953¬µs"
I0119 03:09:47.183918       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="149.214¬µs"
I0119 03:09:48.591078       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="88.129¬µs"
I0119 03:09:49.620962       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="9.830413ms"
I0119 03:09:49.621210       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="69.654¬µs"
I0119 03:09:51.060027       1 node_lifecycle_controller.go:1048] "Controller detected that some Nodes are Ready. Exiting master disruption mode"
I0119 03:25:13.389711       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="NodeNotReady" message="Node minikube status is now: NodeNotReady"
I0119 03:25:13.498961       1 event.go:307] "Event occurred" object="kube-system/kube-apiserver-minikube" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0119 03:25:13.590399       1 event.go:307] "Event occurred" object="kube-system/kube-controller-manager-minikube" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0119 03:25:13.600485       1 event.go:307] "Event occurred" object="kube-system/kube-scheduler-minikube" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0119 03:25:13.610784       1 event.go:307] "Event occurred" object="kube-system/storage-provisioner" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0119 03:25:13.692314       1 event.go:307] "Event occurred" object="kube-system/kube-proxy-7nt22" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0119 03:25:13.709571       1 event.go:307] "Event occurred" object="kube-system/coredns-5dd5756b68-r8hd2" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0119 03:25:13.795568       1 event.go:307] "Event occurred" object="kube-system/etcd-minikube" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0119 03:25:13.799253       1 node_lifecycle_controller.go:1029] "Controller detected that all Nodes are not-Ready. Entering master disruption mode"
I0119 03:25:13.808977       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="98.350162ms"
I0119 03:25:13.809954       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="112.986¬µs"
I0119 03:25:16.131000       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="10.105442ms"
I0119 03:25:16.131307       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="57.604¬µs"
I0119 03:25:28.810144       1 node_lifecycle_controller.go:1048] "Controller detected that some Nodes are Ready. Exiting master disruption mode"
I0119 03:29:12.911585       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="NodeNotReady" message="Node minikube status is now: NodeNotReady"
I0119 03:29:12.933984       1 event.go:307] "Event occurred" object="kube-system/kube-controller-manager-minikube" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0119 03:29:13.022760       1 event.go:307] "Event occurred" object="kube-system/kube-scheduler-minikube" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0119 03:29:13.035445       1 event.go:307] "Event occurred" object="kube-system/storage-provisioner" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0119 03:29:13.046446       1 event.go:307] "Event occurred" object="kube-system/kube-proxy-7nt22" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0119 03:29:13.142065       1 event.go:307] "Event occurred" object="kube-system/coredns-5dd5756b68-r8hd2" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0119 03:29:13.237667       1 event.go:307] "Event occurred" object="kube-system/etcd-minikube" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0119 03:29:13.237895       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="28.149635ms"
I0119 03:29:13.314543       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="142.882¬µs"
I0119 03:29:13.324724       1 event.go:307] "Event occurred" object="kube-system/kube-apiserver-minikube" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0119 03:29:13.324734       1 node_lifecycle_controller.go:1029] "Controller detected that all Nodes are not-Ready. Entering master disruption mode"
I0119 03:29:15.734218       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="7.14369ms"
I0119 03:29:15.734366       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="71.911¬µs"
I0119 03:29:28.332818       1 node_lifecycle_controller.go:1048] "Controller detected that some Nodes are Ready. Exiting master disruption mode"
I0119 04:20:59.270332       1 cleaner.go:175] "Cleaning CSR as it is more than approvedExpiration duration old and approved." csr="csr-c8w5w" approvedExpiration="1h0m0s"
E0119 04:20:59.683618       1 node_lifecycle_controller.go:971] "Error updating node" err="Operation cannot be fulfilled on nodes \"minikube\": the object has been modified; please apply your changes to the latest version and try again" node="minikube"
I0119 04:20:59.769676       1 node_lifecycle_controller.go:1029] "Controller detected that all Nodes are not-Ready. Entering master disruption mode"
I0119 04:21:14.780013       1 node_lifecycle_controller.go:1048] "Controller detected that some Nodes are Ready. Exiting master disruption mode"

* 
* ==> kube-proxy [6f0ca9033a94] <==
* I0119 03:09:47.998036       1 server_others.go:69] "Using iptables proxy"
I0119 03:09:48.031242       1 node.go:141] Successfully retrieved node IP: 192.168.58.2
I0119 03:09:48.118065       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0119 03:09:48.121149       1 server_others.go:152] "Using iptables Proxier"
I0119 03:09:48.121209       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0119 03:09:48.121216       1 server_others.go:438] "Defaulting to no-op detect-local"
I0119 03:09:48.121847       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0119 03:09:48.122549       1 server.go:846] "Version info" version="v1.28.3"
I0119 03:09:48.122606       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0119 03:09:48.126975       1 config.go:188] "Starting service config controller"
I0119 03:09:48.127045       1 shared_informer.go:311] Waiting for caches to sync for service config
I0119 03:09:48.127078       1 config.go:97] "Starting endpoint slice config controller"
I0119 03:09:48.127083       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0119 03:09:48.128705       1 config.go:315] "Starting node config controller"
I0119 03:09:48.128743       1 shared_informer.go:311] Waiting for caches to sync for node config
I0119 03:09:48.227379       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0119 03:09:48.227467       1 shared_informer.go:318] Caches are synced for service config
I0119 03:09:48.229139       1 shared_informer.go:318] Caches are synced for node config
I0119 03:19:53.219568       1 trace.go:236] Trace[1910928102]: "iptables ChainExists" (19-Jan-2024 03:19:48.904) (total time: 4203ms):
Trace[1910928102]: [4.203466646s] [4.203466646s] END
I0119 03:19:53.514711       1 trace.go:236] Trace[1442829591]: "iptables ChainExists" (19-Jan-2024 03:19:48.908) (total time: 3214ms):
Trace[1442829591]: [3.214591357s] [3.214591357s] END

* 
* ==> kube-proxy [ca8b5ab9240b] <==
* I0120 00:09:11.256307       1 server_others.go:69] "Using iptables proxy"
I0120 00:09:11.631553       1 node.go:141] Successfully retrieved node IP: 192.168.58.2
I0120 00:09:12.119682       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0120 00:09:12.123401       1 server_others.go:152] "Using iptables Proxier"
I0120 00:09:12.123552       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0120 00:09:12.123609       1 server_others.go:438] "Defaulting to no-op detect-local"
I0120 00:09:12.128219       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0120 00:09:12.129124       1 server.go:846] "Version info" version="v1.28.3"
I0120 00:09:12.129184       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0120 00:09:12.158056       1 config.go:188] "Starting service config controller"
I0120 00:09:12.163783       1 shared_informer.go:311] Waiting for caches to sync for service config
I0120 00:09:12.211737       1 config.go:315] "Starting node config controller"
I0120 00:09:12.211831       1 shared_informer.go:311] Waiting for caches to sync for node config
I0120 00:09:12.211892       1 config.go:97] "Starting endpoint slice config controller"
I0120 00:09:12.211915       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0120 00:09:12.324100       1 shared_informer.go:318] Caches are synced for node config
I0120 00:09:12.324188       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0120 00:09:12.412663       1 shared_informer.go:318] Caches are synced for service config

* 
* ==> kube-scheduler [28d5cd0baa7a] <==
* I0119 03:09:28.769279       1 serving.go:348] Generated self-signed cert in-memory
W0119 03:09:31.277610       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0119 03:09:31.277674       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0119 03:09:31.277694       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0119 03:09:31.277704       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0119 03:09:31.475559       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I0119 03:09:31.475615       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0119 03:09:31.484760       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0119 03:09:31.559248       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0119 03:09:31.559350       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0119 03:09:31.559704       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
W0119 03:09:31.565636       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0119 03:09:31.565786       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0119 03:09:31.566265       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0119 03:09:31.566005       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0119 03:09:31.566325       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0119 03:09:31.566417       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0119 03:09:31.569082       1 reflector.go:535] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0119 03:09:31.569271       1 reflector.go:147] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0119 03:09:31.571842       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0119 03:09:31.571984       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0119 03:09:31.572179       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0119 03:09:31.572179       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0119 03:09:31.572231       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0119 03:09:31.572238       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0119 03:09:31.572434       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0119 03:09:31.573101       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0119 03:09:31.573148       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0119 03:09:31.573171       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0119 03:09:31.573788       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0119 03:09:31.573953       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0119 03:09:31.575283       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0119 03:09:31.575388       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0119 03:09:31.575514       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0119 03:09:31.575995       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0119 03:09:31.576757       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0119 03:09:31.576857       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0119 03:09:31.576857       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0119 03:09:31.576882       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0119 03:09:31.577441       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0119 03:09:31.577473       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0119 03:09:32.457690       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0119 03:09:32.457755       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0119 03:09:32.794437       1 reflector.go:535] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0119 03:09:32.794658       1 reflector.go:147] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
I0119 03:09:35.660640       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
W0119 03:21:00.440877       1 reflector.go:458] vendor/k8s.io/client-go/informers/factory.go:150: watch of *v1.StorageClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0119 03:21:00.449636       1 reflector.go:458] vendor/k8s.io/client-go/informers/factory.go:150: watch of *v1.Namespace ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0119 03:21:00.452167       1 reflector.go:458] vendor/k8s.io/client-go/informers/factory.go:150: watch of *v1.CSINode ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0119 03:21:00.455334       1 reflector.go:458] vendor/k8s.io/client-go/informers/factory.go:150: watch of *v1.CSIDriver ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0119 03:21:00.456414       1 reflector.go:458] vendor/k8s.io/client-go/informers/factory.go:150: watch of *v1.PersistentVolume ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0119 03:21:00.532926       1 reflector.go:458] vendor/k8s.io/client-go/informers/factory.go:150: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0119 03:21:00.533263       1 reflector.go:458] vendor/k8s.io/client-go/informers/factory.go:150: watch of *v1.StatefulSet ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0119 03:21:00.539744       1 reflector.go:458] vendor/k8s.io/client-go/informers/factory.go:150: watch of *v1.ReplicaSet ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0119 03:21:00.533456       1 reflector.go:458] vendor/k8s.io/client-go/informers/factory.go:150: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0119 03:21:00.554686       1 reflector.go:458] vendor/k8s.io/client-go/informers/factory.go:150: watch of *v1.PersistentVolumeClaim ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0119 03:21:00.541901       1 reflector.go:458] vendor/k8s.io/client-go/informers/factory.go:150: watch of *v1.ReplicationController ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0119 03:21:00.435539       1 reflector.go:458] vendor/k8s.io/client-go/informers/factory.go:150: watch of *v1.CSIStorageCapacity ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0119 03:21:00.650337       1 reflector.go:458] vendor/k8s.io/client-go/informers/factory.go:150: watch of *v1.Pod ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0119 03:21:00.650578       1 reflector.go:458] vendor/k8s.io/client-go/informers/factory.go:150: watch of *v1.PodDisruptionBudget ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding

* 
* ==> kube-scheduler [a5c7f3ee624b] <==
* I0120 00:09:03.122898       1 serving.go:348] Generated self-signed cert in-memory
W0120 00:09:07.152273       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0120 00:09:07.152357       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0120 00:09:07.152367       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0120 00:09:07.152373       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0120 00:09:07.340139       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I0120 00:09:07.340253       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0120 00:09:07.346582       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0120 00:09:07.415547       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0120 00:09:07.415911       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0120 00:09:07.416291       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0120 00:09:07.616470       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kubelet <==
* Jan 20 00:09:02 minikube kubelet[1458]: I0120 00:09:02.954496    1458 kubelet_node_status.go:70] "Attempting to register node" node="minikube"
Jan 20 00:09:02 minikube kubelet[1458]: E0120 00:09:02.955254    1458 kubelet_node_status.go:92] "Unable to register node with API server" err="Post \"https://control-plane.minikube.internal:8443/api/v1/nodes\": dial tcp 192.168.58.2:8443: connect: connection refused" node="minikube"
Jan 20 00:09:06 minikube kubelet[1458]: I0120 00:09:06.176432    1458 kubelet_node_status.go:70] "Attempting to register node" node="minikube"
Jan 20 00:09:07 minikube kubelet[1458]: I0120 00:09:07.520951    1458 kubelet_node_status.go:108] "Node was previously registered" node="minikube"
Jan 20 00:09:07 minikube kubelet[1458]: I0120 00:09:07.521207    1458 kubelet_node_status.go:73] "Successfully registered node" node="minikube"
Jan 20 00:09:07 minikube kubelet[1458]: I0120 00:09:07.526092    1458 kuberuntime_manager.go:1523] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Jan 20 00:09:07 minikube kubelet[1458]: I0120 00:09:07.527605    1458 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Jan 20 00:09:07 minikube kubelet[1458]: I0120 00:09:07.735441    1458 apiserver.go:52] "Watching apiserver"
Jan 20 00:09:07 minikube kubelet[1458]: I0120 00:09:07.742932    1458 topology_manager.go:215] "Topology Admit Handler" podUID="87af21ec-61ea-4391-b258-cd131dc68545" podNamespace="kube-system" podName="storage-provisioner"
Jan 20 00:09:07 minikube kubelet[1458]: I0120 00:09:07.743123    1458 topology_manager.go:215] "Topology Admit Handler" podUID="55011390-4bfe-4fc1-9df8-2b5ae3edb3a2" podNamespace="kube-system" podName="kube-proxy-7nt22"
Jan 20 00:09:07 minikube kubelet[1458]: I0120 00:09:07.743192    1458 topology_manager.go:215] "Topology Admit Handler" podUID="95f6adef-230f-4995-a672-44ea85ea3921" podNamespace="kube-system" podName="coredns-5dd5756b68-r8hd2"
Jan 20 00:09:07 minikube kubelet[1458]: I0120 00:09:07.802294    1458 desired_state_of_world_populator.go:159] "Finished populating initial desired state of world"
Jan 20 00:09:07 minikube kubelet[1458]: I0120 00:09:07.836032    1458 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/55011390-4bfe-4fc1-9df8-2b5ae3edb3a2-lib-modules\") pod \"kube-proxy-7nt22\" (UID: \"55011390-4bfe-4fc1-9df8-2b5ae3edb3a2\") " pod="kube-system/kube-proxy-7nt22"
Jan 20 00:09:07 minikube kubelet[1458]: I0120 00:09:07.836776    1458 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/55011390-4bfe-4fc1-9df8-2b5ae3edb3a2-xtables-lock\") pod \"kube-proxy-7nt22\" (UID: \"55011390-4bfe-4fc1-9df8-2b5ae3edb3a2\") " pod="kube-system/kube-proxy-7nt22"
Jan 20 00:09:07 minikube kubelet[1458]: I0120 00:09:07.836948    1458 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/87af21ec-61ea-4391-b258-cd131dc68545-tmp\") pod \"storage-provisioner\" (UID: \"87af21ec-61ea-4391-b258-cd131dc68545\") " pod="kube-system/storage-provisioner"
Jan 20 00:09:09 minikube kubelet[1458]: I0120 00:09:09.345396    1458 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="edf805ea2e122bca460d0e93dcc35849108603677bdc640d612d18504afafc84"
Jan 20 00:09:12 minikube kubelet[1458]: I0120 00:09:12.948514    1458 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Jan 20 00:09:17 minikube kubelet[1458]: I0120 00:09:17.128314    1458 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Jan 20 00:09:41 minikube kubelet[1458]: I0120 00:09:41.554896    1458 scope.go:117] "RemoveContainer" containerID="109ce8d6b57bd62db2655d9babf4ed4aaf9c55fb484db73f3e4d219d26f0c131"
Jan 20 00:09:41 minikube kubelet[1458]: I0120 00:09:41.555366    1458 scope.go:117] "RemoveContainer" containerID="ec4980a1cf947f8802160e4a14dffe944969578d56f5d404fcc36e815e66a3c7"
Jan 20 00:09:41 minikube kubelet[1458]: E0120 00:09:41.555923    1458 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 10s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(87af21ec-61ea-4391-b258-cd131dc68545)\"" pod="kube-system/storage-provisioner" podUID="87af21ec-61ea-4391-b258-cd131dc68545"
Jan 20 00:09:55 minikube kubelet[1458]: I0120 00:09:55.743251    1458 scope.go:117] "RemoveContainer" containerID="ec4980a1cf947f8802160e4a14dffe944969578d56f5d404fcc36e815e66a3c7"
Jan 20 00:10:15 minikube kubelet[1458]: I0120 00:10:15.764302    1458 topology_manager.go:215] "Topology Admit Handler" podUID="68eb0a37-5ab0-4fef-b14c-143a9d65d95c" podNamespace="default" podName="mongodb-stateful-set-0"
Jan 20 00:10:15 minikube kubelet[1458]: I0120 00:10:15.932940    1458 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pvc-b86435de-6a19-4fad-af24-91adb03bbd48\" (UniqueName: \"kubernetes.io/host-path/68eb0a37-5ab0-4fef-b14c-143a9d65d95c-pvc-b86435de-6a19-4fad-af24-91adb03bbd48\") pod \"mongodb-stateful-set-0\" (UID: \"68eb0a37-5ab0-4fef-b14c-143a9d65d95c\") " pod="default/mongodb-stateful-set-0"
Jan 20 00:10:15 minikube kubelet[1458]: I0120 00:10:15.933094    1458 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-s7mzg\" (UniqueName: \"kubernetes.io/projected/68eb0a37-5ab0-4fef-b14c-143a9d65d95c-kube-api-access-s7mzg\") pod \"mongodb-stateful-set-0\" (UID: \"68eb0a37-5ab0-4fef-b14c-143a9d65d95c\") " pod="default/mongodb-stateful-set-0"
Jan 20 00:10:16 minikube kubelet[1458]: I0120 00:10:16.381366    1458 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="cbf47d6664cd0500b38eac58a672b964f71b1281a142b0925cf2f6dd3425d508"
Jan 20 00:10:20 minikube kubelet[1458]: I0120 00:10:20.761242    1458 topology_manager.go:215] "Topology Admit Handler" podUID="fdaaf1cb-e9e0-42ef-bd82-005e286388b8" podNamespace="default" podName="note-server-deployment-6fb5fcb67f-fln28"
Jan 20 00:10:20 minikube kubelet[1458]: I0120 00:10:20.811353    1458 topology_manager.go:215] "Topology Admit Handler" podUID="2434daf2-0b0d-4fe9-bf12-42006fbabe91" podNamespace="default" podName="note-server-deployment-6fb5fcb67f-nvfsp"
Jan 20 00:10:20 minikube kubelet[1458]: I0120 00:10:20.876624    1458 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-dsxtv\" (UniqueName: \"kubernetes.io/projected/fdaaf1cb-e9e0-42ef-bd82-005e286388b8-kube-api-access-dsxtv\") pod \"note-server-deployment-6fb5fcb67f-fln28\" (UID: \"fdaaf1cb-e9e0-42ef-bd82-005e286388b8\") " pod="default/note-server-deployment-6fb5fcb67f-fln28"
Jan 20 00:10:20 minikube kubelet[1458]: I0120 00:10:20.876715    1458 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-d2jbw\" (UniqueName: \"kubernetes.io/projected/2434daf2-0b0d-4fe9-bf12-42006fbabe91-kube-api-access-d2jbw\") pod \"note-server-deployment-6fb5fcb67f-nvfsp\" (UID: \"2434daf2-0b0d-4fe9-bf12-42006fbabe91\") " pod="default/note-server-deployment-6fb5fcb67f-nvfsp"
Jan 20 00:10:23 minikube kubelet[1458]: I0120 00:10:23.813661    1458 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="5625bc39e4f10cff1589cd9d82447419e773f59507bb82787958c5f76d89130a"
Jan 20 00:10:23 minikube kubelet[1458]: I0120 00:10:23.823444    1458 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="85ad18e960b9f92c77a7b0556698346598c42fa22437e59b906011953526e450"
Jan 20 00:10:55 minikube kubelet[1458]: I0120 00:10:55.008910    1458 topology_manager.go:215] "Topology Admit Handler" podUID="5369ba82-a896-45b5-bd86-e38e18dde580" podNamespace="default" podName="note-deployment-74cc946cd8-kxxtx"
Jan 20 00:10:55 minikube kubelet[1458]: I0120 00:10:55.025366    1458 topology_manager.go:215] "Topology Admit Handler" podUID="7a99c176-947e-4a03-a4b6-3fa9251b145c" podNamespace="default" podName="note-deployment-74cc946cd8-x6smv"
Jan 20 00:10:55 minikube kubelet[1458]: I0120 00:10:55.125885    1458 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-6nbwv\" (UniqueName: \"kubernetes.io/projected/5369ba82-a896-45b5-bd86-e38e18dde580-kube-api-access-6nbwv\") pod \"note-deployment-74cc946cd8-kxxtx\" (UID: \"5369ba82-a896-45b5-bd86-e38e18dde580\") " pod="default/note-deployment-74cc946cd8-kxxtx"
Jan 20 00:10:55 minikube kubelet[1458]: I0120 00:10:55.226696    1458 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-5st5t\" (UniqueName: \"kubernetes.io/projected/7a99c176-947e-4a03-a4b6-3fa9251b145c-kube-api-access-5st5t\") pod \"note-deployment-74cc946cd8-x6smv\" (UID: \"7a99c176-947e-4a03-a4b6-3fa9251b145c\") " pod="default/note-deployment-74cc946cd8-x6smv"
Jan 20 00:11:02 minikube kubelet[1458]: I0120 00:11:02.114221    1458 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="3385c537df1babfe38de2fa64098bc4933bfad4a563250564c0b3fc2ed89777f"
Jan 20 00:11:02 minikube kubelet[1458]: I0120 00:11:02.118687    1458 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="8d0be2dfef1ec2258c571a2627033c6140ae47357f2d0b1acff0f04ed34f2911"
Jan 20 00:11:15 minikube kubelet[1458]: I0120 00:11:15.415261    1458 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/mongodb-stateful-set-0" podStartSLOduration=12.940190911 podCreationTimestamp="2024-01-20 00:10:11 +0000 UTC" firstStartedPulling="2024-01-20 00:10:16.451424747 +0000 UTC m=+77.433964094" lastFinishedPulling="2024-01-20 00:11:07.92426625 +0000 UTC m=+128.905982945" observedRunningTime="2024-01-20 00:11:12.33635035 +0000 UTC m=+133.318067075" watchObservedRunningTime="2024-01-20 00:11:15.412209762 +0000 UTC m=+136.393926447"
Jan 20 00:11:15 minikube kubelet[1458]: I0120 00:11:15.416183    1458 topology_manager.go:215] "Topology Admit Handler" podUID="7f51904c-7236-4585-b38c-2874ebf9648d" podNamespace="default" podName="mongodb-stateful-set-1"
Jan 20 00:11:15 minikube kubelet[1458]: I0120 00:11:15.611786    1458 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-rnczd\" (UniqueName: \"kubernetes.io/projected/7f51904c-7236-4585-b38c-2874ebf9648d-kube-api-access-rnczd\") pod \"mongodb-stateful-set-1\" (UID: \"7f51904c-7236-4585-b38c-2874ebf9648d\") " pod="default/mongodb-stateful-set-1"
Jan 20 00:11:15 minikube kubelet[1458]: I0120 00:11:15.612123    1458 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pvc-d53f0a49-7aba-4eff-9a07-18175bb5fee6\" (UniqueName: \"kubernetes.io/host-path/7f51904c-7236-4585-b38c-2874ebf9648d-pvc-d53f0a49-7aba-4eff-9a07-18175bb5fee6\") pod \"mongodb-stateful-set-1\" (UID: \"7f51904c-7236-4585-b38c-2874ebf9648d\") " pod="default/mongodb-stateful-set-1"
Jan 20 00:11:19 minikube kubelet[1458]: I0120 00:11:19.829374    1458 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="3025a50ed04b29f8c82464d648251c20248518ab32e548d5ab5d7c6daf9664e1"
Jan 20 00:12:21 minikube kubelet[1458]: I0120 00:12:21.922888    1458 topology_manager.go:215] "Topology Admit Handler" podUID="44a18580-ef8a-43c6-a6d1-c67dbb7dc036" podNamespace="default" podName="mongo-express-deployment-b88f6d45f-k2hkj"
Jan 20 00:12:22 minikube kubelet[1458]: I0120 00:12:22.044062    1458 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-kjtxt\" (UniqueName: \"kubernetes.io/projected/44a18580-ef8a-43c6-a6d1-c67dbb7dc036-kube-api-access-kjtxt\") pod \"mongo-express-deployment-b88f6d45f-k2hkj\" (UID: \"44a18580-ef8a-43c6-a6d1-c67dbb7dc036\") " pod="default/mongo-express-deployment-b88f6d45f-k2hkj"
Jan 20 00:12:24 minikube kubelet[1458]: I0120 00:12:24.734389    1458 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="f034fb02521dce477893025ae0ea686406e0ac9dde6805a4f82eba7a8cebda31"
Jan 20 00:12:55 minikube kubelet[1458]: I0120 00:12:55.017931    1458 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/note-server-deployment-6fb5fcb67f-fln28" podStartSLOduration=8.152350428 podCreationTimestamp="2024-01-20 00:10:20 +0000 UTC" firstStartedPulling="2024-01-20 00:10:23.940863174 +0000 UTC m=+84.923402525" lastFinishedPulling="2024-01-20 00:12:50.701806047 +0000 UTC m=+231.686576661" observedRunningTime="2024-01-20 00:12:54.915151334 +0000 UTC m=+235.899921960" watchObservedRunningTime="2024-01-20 00:12:54.915524564 +0000 UTC m=+235.900295190"
Jan 20 00:13:11 minikube kubelet[1458]: I0120 00:13:11.591571    1458 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/note-deployment-74cc946cd8-kxxtx" podStartSLOduration=9.713821552 podCreationTimestamp="2024-01-20 00:10:54 +0000 UTC" firstStartedPulling="2024-01-20 00:11:02.330891232 +0000 UTC m=+123.314733306" lastFinishedPulling="2024-01-20 00:13:10.469249432 +0000 UTC m=+251.192084077" observedRunningTime="2024-01-20 00:13:11.585808452 +0000 UTC m=+252.308643105" watchObservedRunningTime="2024-01-20 00:13:11.591172323 +0000 UTC m=+252.314006963"
Jan 20 00:13:11 minikube kubelet[1458]: I0120 00:13:11.593069    1458 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/note-server-deployment-6fb5fcb67f-nvfsp" podStartSLOduration=21.950677292 podCreationTimestamp="2024-01-20 00:10:20 +0000 UTC" firstStartedPulling="2024-01-20 00:10:23.944275632 +0000 UTC m=+84.926814979" lastFinishedPulling="2024-01-20 00:12:53.584274032 +0000 UTC m=+234.569044648" observedRunningTime="2024-01-20 00:12:55.031227182 +0000 UTC m=+236.015997822" watchObservedRunningTime="2024-01-20 00:13:11.592906961 +0000 UTC m=+252.315741614"
Jan 20 00:13:13 minikube kubelet[1458]: I0120 00:13:13.025834    1458 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/note-deployment-74cc946cd8-x6smv" podStartSLOduration=11.955989595 podCreationTimestamp="2024-01-20 00:10:54 +0000 UTC" firstStartedPulling="2024-01-20 00:11:02.330856376 +0000 UTC m=+123.314698444" lastFinishedPulling="2024-01-20 00:13:09.661657689 +0000 UTC m=+250.384492337" observedRunningTime="2024-01-20 00:13:11.7758852 +0000 UTC m=+252.498719855" watchObservedRunningTime="2024-01-20 00:13:13.025783488 +0000 UTC m=+253.748618135"
Jan 20 00:13:35 minikube kubelet[1458]: E0120 00:13:35.718362    1458 kubelet.go:2477] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.12s"
Jan 20 00:13:39 minikube kubelet[1458]: I0120 00:13:39.509770    1458 scope.go:117] "RemoveContainer" containerID="ec4980a1cf947f8802160e4a14dffe944969578d56f5d404fcc36e815e66a3c7"
Jan 20 00:13:39 minikube kubelet[1458]: I0120 00:13:39.520410    1458 scope.go:117] "RemoveContainer" containerID="b135140aef57692bdd92cbe7cae1eaa8c4eb784e8e14a0c2d5fd42238c737161"
Jan 20 00:13:39 minikube kubelet[1458]: E0120 00:13:39.544128    1458 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 20s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(87af21ec-61ea-4391-b258-cd131dc68545)\"" pod="kube-system/storage-provisioner" podUID="87af21ec-61ea-4391-b258-cd131dc68545"
Jan 20 00:13:39 minikube kubelet[1458]: I0120 00:13:39.727016    1458 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/mongodb-stateful-set-1" podStartSLOduration=36.005610099 podCreationTimestamp="2024-01-20 00:11:13 +0000 UTC" firstStartedPulling="2024-01-20 00:11:20.137032129 +0000 UTC m=+141.118748813" lastFinishedPulling="2024-01-20 00:13:11.117172577 +0000 UTC m=+251.840007220" observedRunningTime="2024-01-20 00:13:13.026239219 +0000 UTC m=+253.749073872" watchObservedRunningTime="2024-01-20 00:13:39.726868506 +0000 UTC m=+280.712716131"
Jan 20 00:13:52 minikube kubelet[1458]: I0120 00:13:52.737984    1458 scope.go:117] "RemoveContainer" containerID="b135140aef57692bdd92cbe7cae1eaa8c4eb784e8e14a0c2d5fd42238c737161"
Jan 20 00:13:52 minikube kubelet[1458]: E0120 00:13:52.738406    1458 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 20s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(87af21ec-61ea-4391-b258-cd131dc68545)\"" pod="kube-system/storage-provisioner" podUID="87af21ec-61ea-4391-b258-cd131dc68545"
Jan 20 00:14:00 minikube kubelet[1458]: W0120 00:14:00.420862    1458 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 20 00:14:03 minikube kubelet[1458]: I0120 00:14:03.739321    1458 scope.go:117] "RemoveContainer" containerID="b135140aef57692bdd92cbe7cae1eaa8c4eb784e8e14a0c2d5fd42238c737161"
Jan 20 00:14:48 minikube kubelet[1458]: I0120 00:14:48.781945    1458 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/mongo-express-deployment-b88f6d45f-k2hkj" podStartSLOduration=6.023897149 podCreationTimestamp="2024-01-20 00:12:21 +0000 UTC" firstStartedPulling="2024-01-20 00:12:24.939433744 +0000 UTC m=+205.923781061" lastFinishedPulling="2024-01-20 00:14:46.694992384 +0000 UTC m=+347.681721270" observedRunningTime="2024-01-20 00:14:48.781000653 +0000 UTC m=+349.767729431" watchObservedRunningTime="2024-01-20 00:14:48.781837358 +0000 UTC m=+349.768566132"

* 
* ==> storage-provisioner [81b2e61bc8fd] <==
* I0120 00:14:06.667015       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0120 00:14:06.756367       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0120 00:14:06.758091       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0120 00:14:24.255833       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0120 00:14:24.256290       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"59ee4cfe-0d71-41b1-95d1-a3055e570af0", APIVersion:"v1", ResourceVersion:"1236", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_5fa4acd0-fab4-447d-8dd3-597b6ec8af39 became leader
I0120 00:14:24.257718       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_5fa4acd0-fab4-447d-8dd3-597b6ec8af39!
I0120 00:14:24.364362       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_5fa4acd0-fab4-447d-8dd3-597b6ec8af39!

* 
* ==> storage-provisioner [b135140aef57] <==
* sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1.3()
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x5c
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0xc00064aa20)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:155 +0x5f
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc00064aa20, 0x18b3d60, 0xc000282000, 0x1, 0xc00014ac00)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:156 +0x9b
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc00064aa20, 0x3b9aca00, 0x0, 0x1, 0xc00014ac00)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:133 +0x98
k8s.io/apimachinery/pkg/util/wait.Until(0xc00064aa20, 0x3b9aca00, 0xc00014ac00)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:90 +0x4d
created by sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x3d6

goroutine 135 [sync.Cond.Wait, 3 minutes]:
sync.runtime_notifyListWait(0xc000047c10, 0xc000000006)
	/usr/local/go/src/runtime/sema.go:513 +0xf8
sync.(*Cond).Wait(0xc000047c00)
	/usr/local/go/src/sync/cond.go:56 +0x99
k8s.io/client-go/util/workqueue.(*Type).Get(0xc00040fbc0, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/util/workqueue/queue.go:145 +0x89
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).processNextClaimWorkItem(0xc000142f00, 0x18e5530, 0xc000047dc0, 0x203001)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:935 +0x3e
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).runClaimWorker(...)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:924
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1.2()
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:880 +0x5c
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0xc00064aa40)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:155 +0x5f
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc00064aa40, 0x18b3d60, 0xc000282030, 0x18b4601, 0xc00014ac00)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:156 +0x9b
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc00064aa40, 0x3b9aca00, 0x0, 0xc000614801, 0xc00014ac00)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:133 +0x98
k8s.io/apimachinery/pkg/util/wait.Until(0xc00064aa40, 0x3b9aca00, 0xc00014ac00)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:90 +0x4d
created by sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:880 +0x4af

goroutine 136 [sync.Cond.Wait, 2 minutes]:
sync.runtime_notifyListWait(0xc000047c50, 0x7)
	/usr/local/go/src/runtime/sema.go:513 +0xf8
sync.(*Cond).Wait(0xc000047c40)
	/usr/local/go/src/sync/cond.go:56 +0x99
k8s.io/client-go/util/workqueue.(*Type).Get(0xc00040fd40, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/util/workqueue/queue.go:145 +0x89
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).processNextVolumeWorkItem(0xc000142f00, 0x18e5530, 0xc000047dc0, 0x203001)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:990 +0x3e
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).runVolumeWorker(...)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:929
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1.3()
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x5c
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0xc00064aa60)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:155 +0x5f
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc00064aa60, 0x18b3d60, 0xc000282120, 0x1, 0xc00014ac00)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:156 +0x9b
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc00064aa60, 0x3b9aca00, 0x0, 0x1, 0xc00014ac00)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:133 +0x98
k8s.io/apimachinery/pkg/util/wait.Until(0xc00064aa60, 0x3b9aca00, 0xc00014ac00)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:90 +0x4d
created by sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x3d6

